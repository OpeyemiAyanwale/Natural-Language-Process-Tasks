{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise sheet 7 with the  emotion dataset.csv and seed words.json Load the data set into your console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# They contain a corpus of Tweets that contain labels for a emotion classification task (anger, joy, sadness and optimism) and a list of possible seed words for each emotion to initialize an LDA with. \n",
    "\n",
    "# In a seeded LDA, we force the model to not find topics itself, but the topics we plant into its training using the seed words. \n",
    "\n",
    "# Using words referring to certain emotions, we force our LDA to model emotions rather than the content/topics of the Tweets. We will analyze, how we can utilize an unsupervised or a seeded LDA for text classification.\n",
    "\n",
    "# As this is not a default task for topic modeling, we cannot use the popular implementation but have to refer back to niche implementations, which is common for more niche NLP-tasks. \n",
    "\n",
    "# For R, you can use the “seededlda” package, which contains decent documentation in its help-pages. \n",
    "# For Python, you will find the file lda model.py in moodle, which is a condensed version of this GitHub repository. See the recommended functions below for more help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lda\n",
      "  Using cached lda-3.0.0.tar.gz (165 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.13.0 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from lda) (1.26.2)\n",
      "Building wheels for collected packages: lda\n",
      "  Building wheel for lda (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for lda \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[50 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The Meson build system\n",
      "  \u001b[31m   \u001b[0m Version: 1.7.0\n",
      "  \u001b[31m   \u001b[0m Source dir: /private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-install-7n8f84l2/lda_e1be66f592d54c6da07cbb55f7c5e520\n",
      "  \u001b[31m   \u001b[0m Build dir: /private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-install-7n8f84l2/lda_e1be66f592d54c6da07cbb55f7c5e520/build\n",
      "  \u001b[31m   \u001b[0m Build type: native build\n",
      "  \u001b[31m   \u001b[0m Project name: lda\n",
      "  \u001b[31m   \u001b[0m Project version: undefined\n",
      "  \u001b[31m   \u001b[0m C compiler for the host machine: cc (clang 15.0.0 \"Apple clang version 15.0.0 (clang-1500.3.9.4)\")\n",
      "  \u001b[31m   \u001b[0m C linker for the host machine: cc ld64 1053.12\n",
      "  \u001b[31m   \u001b[0m Cython compiler for the host machine: cython (cython 3.0.11)\n",
      "  \u001b[31m   \u001b[0m Host machine cpu family: aarch64\n",
      "  \u001b[31m   \u001b[0m Host machine cpu: aarch64\n",
      "  \u001b[31m   \u001b[0m Program python3 found: YES (/Library/Developer/CommandLineTools/usr/bin/python3)\n",
      "  \u001b[31m   \u001b[0m Did not find pkg-config by name 'pkg-config'\n",
      "  \u001b[31m   \u001b[0m Found pkg-config: NO\n",
      "  \u001b[31m   \u001b[0m Run-time dependency python found: NO (tried pkgconfig, pkgconfig and sysconfig)\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m meson.build:8:22: ERROR: Python dependency not found\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m A full log can be found at /private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-install-7n8f84l2/lda_e1be66f592d54c6da07cbb55f7c5e520/build/meson-logs/meson-log.txt\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-install-7n8f84l2/lda_e1be66f592d54c6da07cbb55f7c5e520/build.py\", line 39, in <module>\n",
      "  \u001b[31m   \u001b[0m     build()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-install-7n8f84l2/lda_e1be66f592d54c6da07cbb55f7c5e520/build.py\", line 33, in build\n",
      "  \u001b[31m   \u001b[0m     _meson(\"setup\", BUILD_DIR.as_posix())\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-install-7n8f84l2/lda_e1be66f592d54c6da07cbb55f7c5e520/build.py\", line 17, in _meson\n",
      "  \u001b[31m   \u001b[0m     subprocess.check_call([\"meson\", *list(args)])\n",
      "  \u001b[31m   \u001b[0m   File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py\", line 373, in check_call\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, cmd)\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['meson', 'setup', '/private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-install-7n8f84l2/lda_e1be66f592d54c6da07cbb55f7c5e520/build']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/oayanwale/Library/Python/3.9/lib/python/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/oayanwale/Library/Python/3.9/lib/python/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/oayanwale/Library/Python/3.9/lib/python/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 280, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return _build_backend().build_wheel(\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-build-env-t8x3ie7f/overlay/lib/python3.9/site-packages/poetry/core/masonry/api.py\", line 58, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return WheelBuilder.make_in(\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-build-env-t8x3ie7f/overlay/lib/python3.9/site-packages/poetry/core/masonry/builders/wheel.py\", line 91, in make_in\n",
      "  \u001b[31m   \u001b[0m     wb.build(target_dir=directory)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-build-env-t8x3ie7f/overlay/lib/python3.9/site-packages/poetry/core/masonry/builders/wheel.py\", line 130, in build\n",
      "  \u001b[31m   \u001b[0m     self._build(zip_file)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-build-env-t8x3ie7f/overlay/lib/python3.9/site-packages/poetry/core/masonry/builders/wheel.py\", line 179, in _build\n",
      "  \u001b[31m   \u001b[0m     self._run_build_script(self._package.build_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cf/grsl9dbx6z9ckwfcv128r6hh0000gq/T/pip-build-env-t8x3ie7f/overlay/lib/python3.9/site-packages/poetry/core/masonry/builders/wheel.py\", line 300, in _run_build_script\n",
      "  \u001b[31m   \u001b[0m     subprocess.check_call([self.executable.as_posix(), build_script])\n",
      "  \u001b[31m   \u001b[0m   File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py\", line 373, in check_call\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, cmd)\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['/Library/Developer/CommandLineTools/usr/bin/python3', 'build.py']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for lda\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build lda\n",
      "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (lda)\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  “Worry is a down payment on a problem you may ...  optimism\n",
      "1  My roommate: it's okay that we can't spell bec...     anger\n",
      "2  No but that's so cute. Atsu was probably shy a...       joy\n",
      "3  Rooneys fucking untouchable isn't he? Been fuc...     anger\n",
      "4  it's pretty depressing when u hit pan on ur fa...   sadness\n",
      "{'anger': ['outrageous', 'infuriating', 'ridiculous', 'absurd', 'exasperating', 'disgusting', 'insulting', 'offensive', 'intolerable', 'unacceptable', 'outrage', 'insane', 'angry', 'upset', 'boiling', 'seething', 'frustrated', 'mad', 'irritated', 'livid', 'indignant', 'agitated', 'annoyed', 'pissed', 'irate', 'aggravated', 'enraged', 'bitter', 'displeased', 'disgruntled', 'vexed', 'temperamental', 'cross', 'testy', 'impatient', 'belligerent', 'furious', 'hostile', 'offended', 'exasperated', 'resentful'], 'sadness': ['sad', 'unhappy', 'melancholy', 'dejected', 'mournful', 'downcast', 'despondent', 'blue', 'dismal', 'gloomy', 'forlorn', 'heartbroken', 'woeful', 'crestfallen', 'disheartened', 'grief', 'sorrowful', 'tearful', 'somber', 'bereaved', 'lamenting', 'doleful', 'mournful', 'lugubrious', 'pensive', 'heavyhearted', 'woebegone', 'troubled', 'depressed', 'brokenhearted', 'weepy', 'funereal', 'downhearted', 'low-spirited', 'sullen', 'despairing', 'disconsolate', 'downtrodden', 'dejected', 'wretched', 'grief', 'painful', 'miserable', 'anguished', 'regretful'], 'joy': ['happy', 'joyful', 'delighted', 'cheerful', 'ecstatic', 'exuberant', 'jovial', 'elated', 'content', 'pleased', 'gleeful', 'upbeat', 'excited', 'overjoyed', 'radiant', 'smiling', 'grateful', 'blissful', 'euphoric', 'lighthearted', 'satisfied', 'thrilled', 'bubbly', 'vibrant', 'exhilarated', 'festive', 'merry', 'sunny', 'jubilant', 'carefree', 'optimistic', 'sparkling', 'heartwarming', 'buoyant', 'blessed', 'uplifting', 'cheery', 'grinning', 'animated', 'satisfying', 'delightful', 'smiley', 'gleaming', 'positive', 'wonderful'], 'optimism': ['optimistic', 'hopeful', 'positive', 'encouraging', 'upbeat', 'confident', 'cheerful', 'buoyant', 'inspiring', 'joyful', 'assured', 'sanguine', 'expectant', 'enthusiastic', 'confident', 'uplifting', 'auspicious', 'bright', 'promising', 'hope-filled', 'sunny', 'assuring', 'heartening', 'favorable', 'constructive', 'forward-looking', 'high-spirited', 'hope-giving', 'rosy', 'promising', 'confident', 'reassuring', 'promising', 'reassured', 'promising', 'encouraged', 'spirited', 'promising', 'inspired', 'promising', 'hopeful', 'promising', 'positive', 'promising', 'upbeat']}\n"
     ]
    }
   ],
   "source": [
    "# Task 1: load dataset\n",
    "# option 1\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the Tweets dataset\n",
    "tweets_df = pd.read_csv('/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/emotion_dataset.csv')\n",
    "\n",
    "# Load the seed words JSON file\n",
    "with open('/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/seed_words.json') as f:\n",
    "    seed_words = json.load(f)\n",
    "\n",
    "# Display first few rows of tweets and seed words for verification\n",
    "print(tweets_df.head())\n",
    "print(seed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  “Worry is a down payment on a problem you may ...  optimism\n",
      "1  My roommate: it's okay that we can't spell bec...     anger\n",
      "2  No but that's so cute. Atsu was probably shy a...       joy\n",
      "3  Rooneys fucking untouchable isn't he? Been fuc...     anger\n",
      "4  it's pretty depressing when u hit pan on ur fa...   sadness\n",
      "{'anger': ['outrageous', 'infuriating', 'ridiculous', 'absurd', 'exasperating', 'disgusting', 'insulting', 'offensive', 'intolerable', 'unacceptable', 'outrage', 'insane', 'angry', 'upset', 'boiling', 'seething', 'frustrated', 'mad', 'irritated', 'livid', 'indignant', 'agitated', 'annoyed', 'pissed', 'irate', 'aggravated', 'enraged', 'bitter', 'displeased', 'disgruntled', 'vexed', 'temperamental', 'cross', 'testy', 'impatient', 'belligerent', 'furious', 'hostile', 'offended', 'exasperated', 'resentful'], 'sadness': ['sad', 'unhappy', 'melancholy', 'dejected', 'mournful', 'downcast', 'despondent', 'blue', 'dismal', 'gloomy', 'forlorn', 'heartbroken', 'woeful', 'crestfallen', 'disheartened', 'grief', 'sorrowful', 'tearful', 'somber', 'bereaved', 'lamenting', 'doleful', 'mournful', 'lugubrious', 'pensive', 'heavyhearted', 'woebegone', 'troubled', 'depressed', 'brokenhearted', 'weepy', 'funereal', 'downhearted', 'low-spirited', 'sullen', 'despairing', 'disconsolate', 'downtrodden', 'dejected', 'wretched', 'grief', 'painful', 'miserable', 'anguished', 'regretful'], 'joy': ['happy', 'joyful', 'delighted', 'cheerful', 'ecstatic', 'exuberant', 'jovial', 'elated', 'content', 'pleased', 'gleeful', 'upbeat', 'excited', 'overjoyed', 'radiant', 'smiling', 'grateful', 'blissful', 'euphoric', 'lighthearted', 'satisfied', 'thrilled', 'bubbly', 'vibrant', 'exhilarated', 'festive', 'merry', 'sunny', 'jubilant', 'carefree', 'optimistic', 'sparkling', 'heartwarming', 'buoyant', 'blessed', 'uplifting', 'cheery', 'grinning', 'animated', 'satisfying', 'delightful', 'smiley', 'gleaming', 'positive', 'wonderful'], 'optimism': ['optimistic', 'hopeful', 'positive', 'encouraging', 'upbeat', 'confident', 'cheerful', 'buoyant', 'inspiring', 'joyful', 'assured', 'sanguine', 'expectant', 'enthusiastic', 'confident', 'uplifting', 'auspicious', 'bright', 'promising', 'hope-filled', 'sunny', 'assuring', 'heartening', 'favorable', 'constructive', 'forward-looking', 'high-spirited', 'hope-giving', 'rosy', 'promising', 'confident', 'reassuring', 'promising', 'reassured', 'promising', 'encouraged', 'spirited', 'promising', 'inspired', 'promising', 'hopeful', 'promising', 'positive', 'promising', 'upbeat']}\n"
     ]
    }
   ],
   "source": [
    "# option 2\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define file paths\n",
    "data_folder = \"/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data\"\n",
    "emotion_data_path = f\"{data_folder}/emotion_dataset.csv\"\n",
    "seed_words_path = f\"{data_folder}/seed_words.json\"\n",
    "\n",
    "# Load dataset\n",
    "emotion_df = pd.read_csv(emotion_data_path)\n",
    "print(emotion_df.head())  # Check structure\n",
    "\n",
    "# Load seed words\n",
    "with open(seed_words_path, 'r') as f:\n",
    "    seed_words = json.load(f)\n",
    "print(seed_words)  # Check structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "# Preprocess the texts so that they are fit for an analysis. Argue the use the preprocessing steps you take for the given analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1 Task 2: Preprocess Texts\n",
    "The preprocessing steps might include:\n",
    "\n",
    "Lowercasing: Convert all text to lowercase.\n",
    "Removing Punctuation and Numbers: Keep only alphabetic characters.\n",
    "Tokenization: Split text into individual words.\n",
    "Stop Word Removal: Remove common words that do not contribute much meaning (like \"and\", \"the\", etc.).\n",
    "Lemmatization/Stemming: Reduce words to their base or root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  “Worry is a down payment on a problem you may ...   \n",
      "1  My roommate: it's okay that we can't spell bec...   \n",
      "2  No but that's so cute. Atsu was probably shy a...   \n",
      "3  Rooneys fucking untouchable isn't he? Been fuc...   \n",
      "4  it's pretty depressing when u hit pan on ur fa...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  worry payment problem may never joyce meyer mo...  \n",
      "1  roommate okay spell autocorrect terrible first...  \n",
      "2     cute atsu probably shy photo cherry helped uwu  \n",
      "3  rooneys fucking untouchable fucking dreadful d...  \n",
      "4  pretty depressing u hit pan ur favourite highl...  \n"
     ]
    }
   ],
   "source": [
    "# option 1\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers, keeping only alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize (split into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize remaining words\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Apply preprocessing to tweet texts column (assuming it's named 'text')\n",
    "tweets_df['processed_text'] = tweets_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display processed texts for verification\n",
    "print(tweets_df[['text', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2 Task 2: Preprocessing\n",
    "Since we're using LDA, we need to preprocess the tweets while preserving sentiment-related words.\n",
    "\n",
    "Preprocessing Steps:\n",
    "✅ Lowercasing (keeps words consistent)\n",
    "✅ Removing punctuation & special characters\n",
    "✅ Tokenization (splitting words)\n",
    "✅ Removing stopwords (to keep only meaningful words)\n",
    "✅ Lemmatization (keeps word meanings but standardizes forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  “Worry is a down payment on a problem you may ...   \n",
      "1  My roommate: it's okay that we can't spell bec...   \n",
      "2  No but that's so cute. Atsu was probably shy a...   \n",
      "3  Rooneys fucking untouchable isn't he? Been fuc...   \n",
      "4  it's pretty depressing when u hit pan on ur fa...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  worry payment problem may never joyce meyer mo...  \n",
      "1  roommate okay spell autocorrect terrible first...  \n",
      "2     cute atsu probably shy photo cherry helped uwu  \n",
      "3  rooneys fucking untouchable fucking dreadful d...  \n",
      "4  pretty depressing u hit pan ur favourite highl...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# option 2 \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure required downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers, keeping only alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize (split into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize remaining words\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Apply preprocessing to the 'text' column of the DataFrame\n",
    "emotion_df['processed_text'] = emotion_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Verify that 'processed_text' has been created successfully\n",
    "print(emotion_df[['text', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "# Train five LDAs with K = 4 topics on your texts. For Python, set n features=10000 and set a high n iter-value. For R, you will need the quanteda::dfm function (see the documentation of seededlda::textmodel lda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in emotion_df: Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset again (if necessary)\n",
    "emotion_df = pd.read_csv('/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/emotion_dataset.csv')\n",
    "\n",
    "# Print available columns\n",
    "print(\"Columns in emotion_df:\", emotion_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlda_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LDA_Model  \u001b[38;5;66;03m# Import your custom LDA model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convert processed texts into lists of tokenized words.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenized_texts \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tweets_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/Downloads/NLP_Exercise_24_25/Data/lda_model.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lda\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m      9\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lda'"
     ]
    }
   ],
   "source": [
    "#option 1\n",
    "\n",
    "from lda_model import LDA_Model  # Import your custom LDA model\n",
    "\n",
    "# Convert processed texts into lists of tokenized words.\n",
    "tokenized_texts = [text.split() for text in tweets_df[\"processed_text\"]]\n",
    "\n",
    "# Train 5 LDA models with K=4 topics.\n",
    "lda_models = []\n",
    "for i in range(5):\n",
    "    lda_model = LDA_Model(documents=tokenized_texts)  # Pass tokenized texts directly during initialization.\n",
    "    lda_model.documents_to_topic_model(n_topics=4, n_features=10000, n_iter=200)  # Adjust iterations as needed.\n",
    "    lda_models.append(lda_model)\n",
    "\n",
    "# Display topics for the first trained model.\n",
    "print(\"Topics from first LDA model:\")\n",
    "lda_models[0].display_topics(n_top_words=5)  # Display top words per topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2 Task 3: Train Five LDA Models (Unsupervised)\n",
    "We train five LDA models with K=4 topics (one for each emotion: anger, joy, sadness, optimism).\n",
    "We could not use lda_model.py as required. so (instead we use gensim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.051*\"user\" + 0.010*\"like\" + 0.008*\"get\" + 0.006*\"watch\" + 0.005*\"feel\"')\n",
      "(1, '0.043*\"user\" + 0.010*\"n\" + 0.010*\"amp\" + 0.008*\"like\" + 0.008*\"people\"')\n",
      "(2, '0.088*\"user\" + 0.006*\"like\" + 0.005*\"one\" + 0.004*\"get\" + 0.004*\"day\"')\n",
      "(3, '0.034*\"user\" + 0.010*\"u\" + 0.006*\"get\" + 0.005*\"one\" + 0.005*\"day\"')\n"
     ]
    }
   ],
   "source": [
    "# option 2 Task 3: Train LDA Models\n",
    "# Use Gensim's LDA Implementation as lda_model was giving error\n",
    "\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Assuming you've already preprocessed your tweets and stored them in emotion_df['processed_text']\n",
    "# Convert processed texts into lists of tokenized words.\n",
    "tokenized_texts = [text.split() for text in emotion_df['processed_text']]\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "\n",
    "# Create a bag-of-words corpus from dictionary representation.\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "# Train an LDA model with K=4 topics.\n",
    "lda_model = models.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=200)\n",
    "\n",
    "# Display topics found by the model:\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# option 3 Task 3: Train LDA Models\n",
    "lda module is not installed in your Python environment. Instead of using the lda package, we can use a different library such as sklearn.decomposition.LatentDirichletAllocation (LDA from scikit-learn) to train the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for model 1:\n",
      "  Topic 1:  ['user', 'like', 'live', 'feel', 'watch', 'day', 'amazing', 'one', 'get', 'love']\n",
      "  Topic 2:  ['user', 'like', 'people', 'get', 'amp', 'fucking', 'know', 'way', 'sad', 'even']\n",
      "  Topic 3:  ['user', 'like', 'amp', 'let', 'get', 'today', 'people', 'still', 'one', 'know']\n",
      "  Topic 4:  ['user', 'go', 'get', 'amp', 'tell', 'someone', 'life', 'want', 'people', 'time']\n",
      "Top words for model 2:\n",
      "  Topic 1:  ['user', 'like', 'live', 'feel', 'watch', 'day', 'amazing', 'one', 'get', 'love']\n",
      "  Topic 2:  ['user', 'like', 'people', 'get', 'amp', 'fucking', 'know', 'way', 'sad', 'even']\n",
      "  Topic 3:  ['user', 'like', 'amp', 'let', 'get', 'today', 'people', 'still', 'one', 'know']\n",
      "  Topic 4:  ['user', 'go', 'get', 'amp', 'tell', 'someone', 'life', 'want', 'people', 'time']\n",
      "Top words for model 3:\n",
      "  Topic 1:  ['user', 'like', 'live', 'feel', 'watch', 'day', 'amazing', 'one', 'get', 'love']\n",
      "  Topic 2:  ['user', 'like', 'people', 'get', 'amp', 'fucking', 'know', 'way', 'sad', 'even']\n",
      "  Topic 3:  ['user', 'like', 'amp', 'let', 'get', 'today', 'people', 'still', 'one', 'know']\n",
      "  Topic 4:  ['user', 'go', 'get', 'amp', 'tell', 'someone', 'life', 'want', 'people', 'time']\n",
      "Top words for model 4:\n",
      "  Topic 1:  ['user', 'like', 'live', 'feel', 'watch', 'day', 'amazing', 'one', 'get', 'love']\n",
      "  Topic 2:  ['user', 'like', 'people', 'get', 'amp', 'fucking', 'know', 'way', 'sad', 'even']\n",
      "  Topic 3:  ['user', 'like', 'amp', 'let', 'get', 'today', 'people', 'still', 'one', 'know']\n",
      "  Topic 4:  ['user', 'go', 'get', 'amp', 'tell', 'someone', 'life', 'want', 'people', 'time']\n",
      "Top words for model 5:\n",
      "  Topic 1:  ['user', 'like', 'live', 'feel', 'watch', 'day', 'amazing', 'one', 'get', 'love']\n",
      "  Topic 2:  ['user', 'like', 'people', 'get', 'amp', 'fucking', 'know', 'way', 'sad', 'even']\n",
      "  Topic 3:  ['user', 'like', 'amp', 'let', 'get', 'today', 'people', 'still', 'one', 'know']\n",
      "  Topic 4:  ['user', 'go', 'get', 'amp', 'tell', 'someone', 'life', 'want', 'people', 'time']\n"
     ]
    }
   ],
   "source": [
    "# option 3\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Convert text into document-term matrix (DTM)\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(emotion_df['processed_text'])\n",
    "\n",
    "# Train 5 LDA models with K=4 topics using scikit-learn's LatentDirichletAllocation\n",
    "n_topics = 4\n",
    "n_iter = 1000  # Set a high number of iterations for convergence\n",
    "lda_models = []\n",
    "\n",
    "for i in range(5):\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=n_iter, random_state=42)\n",
    "    lda.fit(X)\n",
    "    lda_models.append(lda)\n",
    "\n",
    "# Display top words for each model\n",
    "for i, lda in enumerate(lda_models):\n",
    "    print(f\"Top words for model {i+1}:\")\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"  Topic {topic_idx + 1}: \", [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that all five LDA models produced very similar topics, which is interesting. Here’s a breakdown of the results and what they might suggest:\n",
    "\n",
    "### Observations:\n",
    "1. **Common Terms**: \n",
    "   - **User-related Words**: The term \"user\" appears prominently across multiple topics, suggesting that many tweets may be directed at or about users, possibly in the context of social media interactions.\n",
    "   - **Emotional Words**: Words like \"feel,\" \"love,\" and \"sad\" indicate an emotional aspect to the tweets, aligning with the sentiment analysis context.\n",
    "\n",
    "2. **Repetitive Topics**: \n",
    "   - The similarity between topics across different models suggests that the data may have limited diversity in terms of themes or that the model is capturing recurring patterns in language used on Twitter.\n",
    "   - This repetition can also indicate that the dataset might contain a lot of similar content or expressions.\n",
    "\n",
    "3. **Potential Overfitting**: If all models yield identical results, it could signal overfitting due to insufficient variation in your training data or parameters not being diverse enough.\n",
    "\n",
    "### Next Steps\n",
    "1. **Evaluate Topic Quality**: Consider whether these topics are meaningful representations of distinct themes within your dataset. You might want to examine some actual tweets from each topic for qualitative insights.\n",
    "   \n",
    "2. **Adjust Parameters**: You could try experimenting with different values for `n_topics`, `n_iter`, and other hyperparameters to see if you can achieve more varied results.\n",
    "\n",
    "3. **Visualize Topics**: Consider visualizing these topics using word clouds or bar charts to better understand their composition visually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Fix 1: Improve Stopword Removal\n",
    "#We need to expand stopwords to remove Twitter artifacts and generic words.\n",
    "\n",
    "ccustom_stop_words = stop_words.union({\n",
    "    \"user\", \"like\", \"amp\", \"n\", \"u\", \"one\", \"get\", \"im\", \"back\", \"day\", \"really\",\n",
    "    \"know\", \"see\", \"would\", \"someone\", \"time\", \"life\", \"good\", \"need\", \"make\", \"watch\"\n",
    "})\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()  # Lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in custom_stop_words]  # Remove stop words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply improved preprocessing\n",
    "emotion_df[\"processed_text\"] = emotion_df[\"text\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.062*\"user\" + 0.008*\"like\" + 0.008*\"n\" + 0.007*\"get\" + 0.005*\"amp\" + 0.005*\"people\" + 0.005*\"u\" + 0.004*\"think\" + 0.004*\"even\" + 0.003*\"feel\"')\n",
      "(1, '0.090*\"user\" + 0.007*\"like\" + 0.005*\"amp\" + 0.005*\"go\" + 0.005*\"one\" + 0.004*\"want\" + 0.004*\"back\" + 0.004*\"u\" + 0.004*\"watch\" + 0.003*\"today\"')\n",
      "(2, '0.016*\"user\" + 0.008*\"like\" + 0.007*\"know\" + 0.006*\"life\" + 0.005*\"n\" + 0.004*\"sad\" + 0.004*\"feel\" + 0.004*\"make\" + 0.004*\"amp\" + 0.004*\"gbbo\"')\n",
      "(3, '0.024*\"user\" + 0.007*\"get\" + 0.006*\"n\" + 0.005*\"time\" + 0.005*\"day\" + 0.005*\"im\" + 0.004*\"fear\" + 0.004*\"lost\" + 0.004*\"depression\" + 0.004*\"anxiety\"')\n"
     ]
    }
   ],
   "source": [
    "# Fix 2: Increase passes & Use alpha='auto' for Better Topic Separation\n",
    "\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Train LDA with improved topic separation\n",
    "lda_model = models.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=500, alpha='auto')\n",
    "\n",
    "# Display topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "# Train a seeded LDA with the seeds from Task 1 on your texts using the same parameters as in Task 3. Look at the model’s top words. Did the seeding work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in emotion_df: Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset again (if necessary)\n",
    "emotion_df = pd.read_csv('/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/emotion_dataset.csv')\n",
    "\n",
    "# Print available columns\n",
    "print(\"Columns in emotion_df:\", emotion_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  “Worry is a down payment on a problem you may ...   \n",
      "1  My roommate: it's okay that we can't spell bec...   \n",
      "2  No but that's so cute. Atsu was probably shy a...   \n",
      "3  Rooneys fucking untouchable isn't he? Been fuc...   \n",
      "4  it's pretty depressing when u hit pan on ur fa...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  worry payment problem may never joyce meyer mo...  \n",
      "1  roommate okay spell autocorrect terrible first...  \n",
      "2     cute atsu probably shy photo cherry helped uwu  \n",
      "3  rooneys fucking untouchable fucking dreadful d...  \n",
      "4  pretty depressing u hit pan ur favourite highl...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#preprocess again if necessary \n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers, keeping only alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize (split into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize remaining words\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Apply preprocessing to the 'text' column of the DataFrame\n",
    "emotion_df['processed_text'] = emotion_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Verify that 'processed_text' has been created successfully\n",
    "print(emotion_df[['text', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics from seeded LDA model:\n",
      "(0, '0.039*\"user\" + 0.010*\"like\" + 0.007*\"time\" + 0.006*\"amp\" + 0.005*\"get\"')\n",
      "(1, '0.120*\"user\" + 0.007*\"amp\" + 0.006*\"like\" + 0.006*\"u\" + 0.005*\"watch\"')\n",
      "(2, '0.010*\"get\" + 0.005*\"anxiety\" + 0.005*\"one\" + 0.005*\"someone\" + 0.005*\"need\"')\n",
      "(3, '0.024*\"user\" + 0.015*\"n\" + 0.007*\"like\" + 0.006*\"day\" + 0.006*\"people\"')\n"
     ]
    }
   ],
   "source": [
    "# option 1\n",
    "\n",
    "import json\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Load the seed words JSON file\n",
    "with open('/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/seed_words.json') as f:\n",
    "    seed_words = json.load(f)\n",
    "\n",
    "# Prepare seed terms based on emotions from seed_words JSON file.\n",
    "seed_terms_per_emotion = {emotion: seeds for emotion, seeds in seed_words.items()}\n",
    "\n",
    "# Convert processed texts into lists of tokenized words (if not done already).\n",
    "tokenized_texts = [text.split() for text in emotion_df['processed_text']]\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "\n",
    "# Create a bag-of-words corpus from dictionary representation.\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "# Train a seeded LDA model (you may need to adapt this part depending on your implementation)\n",
    "# For example, if using Gensim's built-in methods:\n",
    "seeded_lda_model = models.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=200)\n",
    "\n",
    "# Display top terms from seeded model:\n",
    "print(\"Topics from seeded LDA model:\")\n",
    "topics_seeded = seeded_lda_model.print_topics(num_words=5)\n",
    "for topic in topics_seeded:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "# Calculate the most dominant topic for each document for the models from task 3 and 4. \n",
    "# Com- pare the results with the true emotion labels for eacht text by using a confusion matrix. \n",
    "# Which model does better? \n",
    "# Is it good enough or do you think, we need to find a more method more suited to a classifcation task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Function to get dominant topics from an LDA model\n",
    "def get_dominant_topics(model, corpus):\n",
    "    \"\"\"Get dominant topics for each document.\"\"\"\n",
    "    dominant_topics = []\n",
    "    \n",
    "    for doc_bow in corpus:\n",
    "        topic_distribution = model.get_document_topics(doc_bow)\n",
    "        # Find the topic with the highest probability\n",
    "        dominant_topic_id = max(topic_distribution, key=lambda x: x[1])[0]  \n",
    "        dominant_topics.append(dominant_topic_id)\n",
    "        \n",
    "    return dominant_topics\n",
    "\n",
    "# Prepare corpus for both models (use bag-of-words representation)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Regular LDA:\n",
      " [[  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [291 375 454 280   0   0   0   0]\n",
      " [166 126 266 150   0   0   0   0]\n",
      " [ 82  76  75  61   0   0   0   0]\n",
      " [187 239 243 186   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Get dominant topics from regular LDA model (first model)\n",
    "lda_topic_distribution = lda_models[0].transform(X)  # X is your document-term matrix\n",
    "tweets_df['dominant_topic'] = np.argmax(lda_topic_distribution, axis=1)  # Get index of max probability topic\n",
    "\n",
    "# Convert dominant topic predictions to string\n",
    "tweets_df['dominant_topic'] = tweets_df['dominant_topic'].astype(str)\n",
    "\n",
    "# Assuming you have a column 'label' representing true emotions,\n",
    "confusion_mat_lda = confusion_matrix(tweets_df['label'], tweets_df['dominant_topic'])\n",
    "\n",
    "print(\"Confusion Matrix for Regular LDA:\\n\", confusion_mat_lda)\n",
    "\n",
    "\n",
    "# Now repeat similar steps for seeded LDA model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label             object\n",
      "dominant_topic    object\n",
      "dtype: object\n",
      "label                    object\n",
      "seeded_dominant_topic     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types of relevant columns\n",
    "print(tweets_df[['label', 'dominant_topic']].dtypes)\n",
    "\n",
    "# Check data types of relevant columns\n",
    "print(tweets_df[['label', 'seeded_dominant_topic']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Regular LDA:\n",
      " [[  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [291 375 454 280   0   0   0   0]\n",
      " [166 126 266 150   0   0   0   0]\n",
      " [ 82  76  75  61   0   0   0   0]\n",
      " [187 239 243 186   0   0   0   0]]\n",
      "Confusion Matrix for Seeded LDA:\n",
      " [[  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [326 504 249 321   0   0   0   0]\n",
      " [178 287 116 127   0   0   0   0]\n",
      " [ 67  94  72  61   0   0   0   0]\n",
      " [253 260 143 199   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Convert seeded dominant topic predictions to string\n",
    "#tweets_df['dominant_topic'] = tweets_df['dominant_topic'].astype(str)\n",
    "tweets_df['seeded_dominant_topic'] = tweets_df['seeded_dominant_topic'].astype(str)\n",
    "\n",
    "# Now calculate confusion matrices again\n",
    "confusion_mat_lda = confusion_matrix(tweets_df['label'], tweets_df['dominant_topic'])\n",
    "print(\"Confusion Matrix for Regular LDA:\\n\", confusion_mat_lda)\n",
    "\n",
    "confusion_mat_seeded_lda = confusion_matrix(tweets_df['label'], tweets_df['seeded_dominant_topic'])\n",
    "print(\"Confusion Matrix for Seeded LDA:\\n\", confusion_mat_seeded_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "Zero Predictions for Some Classes: The first four rows of both matrices are entirely zeros, indicating that no predictions were made for those categories. This suggests that the model did not identify any documents belonging to these classes.\n",
    "\n",
    "Class Distribution: In both matrices, certain categories have high counts in specific classes but do not seem to correspond well with the actual distribution of emotions. For example, in the seeded LDA matrix, there are substantial counts for some categories (e.g., 326 predicted as one label).\n",
    "\n",
    "Overall Performance: The presence of many zeros suggests that both models may be struggling with classification tasks or that certain emotions may not be well represented in your dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Regular LDA:\n",
      "Accuracy: 0.00\n",
      "Evaluation of Seeded LDA:\n",
      "Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance by comparing matrices\n",
    "def evaluate_confusion_matrix(confusion_matrix):\n",
    "    accuracy = np.trace(confusion_matrix) / np.sum(confusion_matrix)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "print(\"Evaluation of Regular LDA:\")\n",
    "evaluate_confusion_matrix(confusion_mat_lda)\n",
    "\n",
    "print(\"Evaluation of Seeded LDA:\")\n",
    "evaluate_confusion_matrix(confusion_mat_seeded_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text     label  \\\n",
      "0   “Worry is a down payment on a problem you may ...  optimism   \n",
      "1   My roommate: it's okay that we can't spell bec...     anger   \n",
      "2   No but that's so cute. Atsu was probably shy a...       joy   \n",
      "3   Rooneys fucking untouchable isn't he? Been fuc...     anger   \n",
      "4   it's pretty depressing when u hit pan on ur fa...   sadness   \n",
      "5   @user but your pussy was weak from what I hear...     anger   \n",
      "6   Making that yearly transition from excited and...   sadness   \n",
      "7   Tiller and breezy should do a collab album. Ra...       joy   \n",
      "8   @user broadband is shocking regretting signing...     anger   \n",
      "9                 @user Look at those teef! #growl \\n     anger   \n",
      "10  @user @user USA was embarrassing to watch. Whe...     anger   \n",
      "11  #NewYork: Several #Baloch &amp; Indian activis...   sadness   \n",
      "12  Your glee filled Normy dry humping of the most...     anger   \n",
      "13        What a fucking muppet.  @user  #stalker. \\n     anger   \n",
      "14  Autocorrect changes ''em' to 'me' which I rese...     anger   \n",
      "15  @user I would never strategically vote for som...     anger   \n",
      "16  @user Haters!!! You are low in self worth. Sel...     anger   \n",
      "17  I saved him after ordering him to risk his lif...  optimism   \n",
      "18  @user Uggh that's really horrible. You're not ...  optimism   \n",
      "19  @user @user @user Tamra would F her up if she ...     anger   \n",
      "\n",
      "   dominant_topic seeded_dominant_topic  \n",
      "0               3                     0  \n",
      "1               1                     0  \n",
      "2               0                     2  \n",
      "3               1                     3  \n",
      "4               1                     3  \n",
      "5               3                     1  \n",
      "6               2                     2  \n",
      "7               1                     1  \n",
      "8               3                     3  \n",
      "9               2                     0  \n",
      "10              3                     1  \n",
      "11              3                     1  \n",
      "12              2                     1  \n",
      "13              0                     3  \n",
      "14              1                     3  \n",
      "15              3                     1  \n",
      "16              1                     1  \n",
      "17              1                     3  \n",
      "18              1                     1  \n",
      "19              2                     0  \n"
     ]
    }
   ],
   "source": [
    "# Check some examples where predictions were made\n",
    "print(tweets_df[['text', 'label', 'dominant_topic', 'seeded_dominant_topic']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Regular LDA:\n",
      " [[  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [291 375 454 280   0   0   0   0]\n",
      " [166 126 266 150   0   0   0   0]\n",
      " [ 82  76  75  61   0   0   0   0]\n",
      " [187 239 243 186   0   0   0   0]]\n",
      "Confusion Matrix for Seeded LDA:\n",
      " [[  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [326 504 249 321   0   0   0   0]\n",
      " [178 287 116 127   0   0   0   0]\n",
      " [ 67  94  72  61   0   0   0   0]\n",
      " [253 260 143 199   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrix for regular LDA model\n",
    "confusion_mat_lda = confusion_matrix(tweets_df['label'], tweets_df['dominant_topic'])\n",
    "print(\"Confusion Matrix for Regular LDA:\\n\", confusion_mat_lda)\n",
    "\n",
    "# Calculate confusion matrix for seeded LDA model\n",
    "confusion_mat_seeded_lda = confusion_matrix(tweets_df['label'], tweets_df['seeded_dominant_topic'])\n",
    "print(\"Confusion Matrix for Seeded LDA:\\n\", confusion_mat_seeded_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Regular LDA:\n",
      "Accuracy: 0.00\n",
      "Evaluation of Seeded LDA:\n",
      "Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "def evaluate_confusion_matrix(confusion_mat):\n",
    "    \"\"\"Calculate and print accuracy from confusion matrix.\"\"\"\n",
    "    accuracy = np.trace(confusion_mat) / np.sum(confusion_mat)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "print(\"Evaluation of Regular LDA:\")\n",
    "evaluate_confusion_matrix(confusion_mat_lda)\n",
    "\n",
    "print(\"Evaluation of Seeded LDA:\")\n",
    "evaluate_confusion_matrix(confusion_mat_seeded_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text     label  \\\n",
      "0   “Worry is a down payment on a problem you may ...  optimism   \n",
      "1   My roommate: it's okay that we can't spell bec...     anger   \n",
      "2   No but that's so cute. Atsu was probably shy a...       joy   \n",
      "3   Rooneys fucking untouchable isn't he? Been fuc...     anger   \n",
      "4   it's pretty depressing when u hit pan on ur fa...   sadness   \n",
      "5   @user but your pussy was weak from what I hear...     anger   \n",
      "6   Making that yearly transition from excited and...   sadness   \n",
      "7   Tiller and breezy should do a collab album. Ra...       joy   \n",
      "8   @user broadband is shocking regretting signing...     anger   \n",
      "9                 @user Look at those teef! #growl \\n     anger   \n",
      "10  @user @user USA was embarrassing to watch. Whe...     anger   \n",
      "11  #NewYork: Several #Baloch &amp; Indian activis...   sadness   \n",
      "12  Your glee filled Normy dry humping of the most...     anger   \n",
      "13        What a fucking muppet.  @user  #stalker. \\n     anger   \n",
      "14  Autocorrect changes ''em' to 'me' which I rese...     anger   \n",
      "15  @user I would never strategically vote for som...     anger   \n",
      "16  @user Haters!!! You are low in self worth. Sel...     anger   \n",
      "17  I saved him after ordering him to risk his lif...  optimism   \n",
      "18  @user Uggh that's really horrible. You're not ...  optimism   \n",
      "19  @user @user @user Tamra would F her up if she ...     anger   \n",
      "\n",
      "   dominant_topic seeded_dominant_topic  \n",
      "0               3                     0  \n",
      "1               1                     0  \n",
      "2               0                     2  \n",
      "3               1                     3  \n",
      "4               1                     3  \n",
      "5               3                     1  \n",
      "6               2                     2  \n",
      "7               1                     1  \n",
      "8               3                     3  \n",
      "9               2                     0  \n",
      "10              3                     1  \n",
      "11              3                     1  \n",
      "12              2                     1  \n",
      "13              0                     3  \n",
      "14              1                     3  \n",
      "15              3                     1  \n",
      "16              1                     1  \n",
      "17              1                     3  \n",
      "18              1                     1  \n",
      "19              2                     0  \n"
     ]
    }
   ],
   "source": [
    "# Check some examples where predictions were made\n",
    "print(tweets_df[['text', 'label', 'dominant_topic', 'seeded_dominant_topic']].head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
