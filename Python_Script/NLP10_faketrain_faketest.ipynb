{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise sheet 10 with fake train and fake test . Load the data set into your console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "In moodle you will find the files fake train.csv and fake test.csv. They each contain a set of news articles that either contain factual news or fake news.\n",
    "\n",
    "In the upcoming tasks, we will try to differentiate fake news from real news by comparing their document embeddings. \n",
    "\n",
    "For this, we will train a document embedding model on the whole corpus. \n",
    "\n",
    "Then, we will use the embeddings of our train-corpus to train a logistic regression model that tries to predict the labels of the test-corpus given their embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument  # Updated import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "train_file_path = '/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/fake_train.csv'\n",
    "test_file_path = '/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/fake_test.csv'\n",
    "\n",
    "# Task 1: Load data\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 2\n",
    "# Preprocess the texts so that they are fit for an analysis. Argue the use the preprocessing steps you take for the given analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "                                                text  label  \\\n",
      "0  Trump administration to review goal of world w...      1   \n",
      "1  Turkish academics to be tried in April over Ku...      1   \n",
      "2  Factbox: Italy's new electoral law offers a mi...      1   \n",
      "3   WATCH: Trump Get His A** Handed To Him By Chr...      0   \n",
      "4  Mexico president says Trump visit could have b...      1   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  trump administration review goal world without...  \n",
      "1  turkish academic tried april kurdish letter is...  \n",
      "2  factbox italy new electoral law offer mix syst...  \n",
      "3  watch trump get handed chris cuomo cry fake ne...  \n",
      "4  mexico president say trump visit could done be...  \n",
      "\n",
      "Test Data:\n",
      "                                                text  label  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...      1   \n",
      "1  U.S. military to accept transgender recruits o...      1   \n",
      "2  Senior U.S. Republican senator: 'Let Mr. Muell...      1   \n",
      "3  FBI Russia probe helped by Australian diplomat...      1   \n",
      "4  Trump wants Postal Service to charge 'much mor...      1   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  u budget fight loom republican flip fiscal scr...  \n",
      "1  u military accept transgender recruit monday p...  \n",
      "2  senior u republican senator let mr mueller job...  \n",
      "3  fbi russia probe helped australian diplomat ti...  \n",
      "4  trump want postal service charge much amazon s...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize necessary components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenization (split text into individual words)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Remove numbers (optional for news articles)\n",
    "    tokens = [token for token in tokens if not token.isdigit()]\n",
    "    \n",
    "    # Lemmatization (reduce words to their base form)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string and strip excess whitespace\n",
    "    preprocessed_text = ' '.join(tokens).strip()\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing to the 'text' column of both datasets\n",
    "train_df['preprocessed_text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['preprocessed_text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(\"Train Data:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argue the use the preprocessing steps you take for the given analysis.\n",
    "\n",
    "1. Lowercasing\n",
    "Purpose: Converting all text to lowercase ensures uniformity across the dataset.\n",
    "Argument: This step is crucial because it eliminates case sensitivity, allowing words like \"Trump\" and \"trump\" to be treated as the same token. This helps reduce redundancy in the vocabulary and aids in better model performance.\n",
    "\n",
    "2. Removing Punctuation\n",
    "Purpose: Punctuation marks do not typically carry semantic meaning that contributes to understanding or classifying text.\n",
    "Argument: By removing punctuation, you reduce noise in the data. In news articles, punctuation can often interfere with tokenization and may not provide valuable information for classification tasks.\n",
    "\n",
    "3. Tokenization\n",
    "Purpose: Splitting text into individual words (tokens) allows for detailed analysis of word frequencies and relationships.\n",
    "Argument: Tokenization is a foundational step in NLP that enables further processing such as stop word removal and lemmatization. It transforms the continuous string of text into manageable units that can be analyzed individually.\n",
    "\n",
    "4. Removing Stopwords\n",
    "Purpose: Stopwords are common words (e.g., \"and\", \"the\", \"is\") that do not add significant meaning to sentences.\n",
    "Argument: By filtering out stopwords, you focus on more meaningful content within the articles. This reduces dimensionality and helps improve the efficiency of machine learning algorithms by only considering relevant tokens.\n",
    "\n",
    "5. Removing Numbers (Optional)\n",
    "Purpose: Depending on context, numbers may not contribute meaningful information for classification tasks.\n",
    "Argument: In many cases, especially when analyzing textual content rather than numerical data, removing numbers helps clean up the dataset. However, if your analysis involves financial or statistical news where numbers are significant, this step might need reconsideration.\n",
    "\n",
    "6. Lemmatization\n",
    "Purpose: Lemmatization reduces words to their base or root form (e.g., \"running\" becomes \"run\").\n",
    "Argument: This process helps group different forms of a word together under a single representation which is beneficial for understanding semantics and improving model accuracy. It ensures that variations of a word do not inflate feature space unnecessarily.\n",
    "\n",
    "7. Joining Tokens Back into a Single String\n",
    "Purpose: After preprocessing, it’s often useful to convert tokens back into a single string format for certain models or analyses.\n",
    "Argument: Some models require input as strings rather than lists of tokens; thus joining them allows flexibility depending on how subsequent analyses will be performed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "# Train a Doc2Vec model on all documents from both the training and test corpus with a window size of four and a vector dimension of 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# Prepare the tagged data by tagging each document\n",
    "combined_df = pd.concat([train_df[['preprocessed_text']], test_df[['preprocessed_text']]])\n",
    "\n",
    "# Create TaggedDocument for each document in the combined dataset\n",
    "tagged_data = [TaggedDocument(words=row.split(), tags=[str(i)]) for i, row in enumerate(combined_df['preprocessed_text'])]\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=300, window=4, min_count=1, workers=4, epochs=10)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Save the model (optional)\n",
    "model.save(\"/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/doc2vec_model.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = Doc2Vec.load(\"/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/doc2vec_model.model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "# Create a data frame of all document embeddings of the documents within the training corpus and the label of the respective document. Use this data frame to train a logistic regression that uses the embeddings to predict the label of the document.\n",
    "\n",
    "# Use it to predict the labels of all documents in the test-corpus using their embeddings. Compare the resulting labels to the true labels and return the classification rate. How well does the model perform?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.9467476351745884\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create embeddings for each document in the training set\n",
    "train_embeddings = [model.infer_vector(row.split()) for row in train_df['preprocessed_text']]\n",
    "\n",
    "# Add the embeddings and the labels to a new DataFrame\n",
    "train_embeddings_df = pd.DataFrame(train_embeddings)\n",
    "train_embeddings_df['label'] = train_df['label']\n",
    "\n",
    "# Train a Logistic Regression model using the document embeddings\n",
    "X_train = train_embeddings_df.drop('label', axis=1)\n",
    "y_train = train_embeddings_df['label']\n",
    "\n",
    "log_reg_model = LogisticRegression(max_iter=1000)\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Now use the model to predict on the test set\n",
    "test_embeddings = [model.infer_vector(row.split()) for row in test_df['preprocessed_text']]\n",
    "test_embeddings_df = pd.DataFrame(test_embeddings)\n",
    "\n",
    "# Make predictions\n",
    "predictions = log_reg_model.predict(test_embeddings_df)\n",
    "\n",
    "# Compare with true labels\n",
    "accuracy = accuracy_score(test_df['label'], predictions)\n",
    "print(f\"Classification accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "# Repeat tasks 3 and 4 with one adjustment: Train your initial Doc2Vec model only on the train-corpus. This way, the test-corpus is entirely unobserved for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Doc2Vec - Train-only): 0.9822102845575927\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "\n",
    "# Re-train Doc2Vec model using only the train corpus\n",
    "train_docs = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(train_df['text'])]\n",
    "model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=40)\n",
    "model.build_vocab(train_docs)\n",
    "model.train(train_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Generate embeddings for training data\n",
    "train_embeddings = [model.infer_vector(doc.split()) for doc in train_df['text']]\n",
    "\n",
    "# Create DataFrame with embeddings and corresponding labels\n",
    "train_embeddings_df = pd.DataFrame(train_embeddings)\n",
    "train_embeddings_df['label'] = train_df['label']\n",
    "\n",
    "# Train logistic regression\n",
    "X_train = train_embeddings_df.drop('label', axis=1)\n",
    "y_train = train_embeddings_df['label']\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Generate embeddings for test data\n",
    "test_embeddings = [model.infer_vector(doc.split()) for doc in test_df['text']]\n",
    "X_test = pd.DataFrame(test_embeddings)\n",
    "y_test = test_df['label']  # Extract actual labels\n",
    "\n",
    "# Predict labels for test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compare predicted labels with true labels\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Classification Accuracy (Doc2Vec - Train-only): {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "# Repeat tasks 3 and 4 with a different adjustment: Train a Word2Vec model and create a document embedding for each document by averaging all word vector that document contains. \n",
    "# Which of the three models from task 3, 5 and 6 performs best in classifying the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy (Word2Vec): 0.9889446844953093\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train Word2Vec model on the train corpus\n",
    "word2vec_model = Word2Vec(sentences=[doc.split() for doc in train_df['text']], vector_size=100, window=2, min_count=1, workers=4, epochs=40)\n",
    "\n",
    "# Create document embeddings by averaging word embeddings\n",
    "def get_word2vec_embedding(doc):\n",
    "    words = doc.split()\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Generate embeddings for training data\n",
    "train_embeddings = [get_word2vec_embedding(doc) for doc in train_df['text']]\n",
    "\n",
    "# Create DataFrame with embeddings and corresponding labels\n",
    "train_embeddings_df = pd.DataFrame(train_embeddings)\n",
    "train_embeddings_df['label'] = train_df['label']\n",
    "\n",
    "# Train logistic regression\n",
    "X_train = train_embeddings_df.drop('label', axis=1)\n",
    "y_train = train_embeddings_df['label']\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Generate embeddings for test data\n",
    "test_embeddings = [get_word2vec_embedding(doc) for doc in test_df['text']]\n",
    "X_test = pd.DataFrame(test_embeddings)\n",
    "y_test = test_df['label']  # Extract actual labels\n",
    "\n",
    "# Predict labels for test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compare predicted labels with true labels\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Classification Accuracy (Word2Vec): {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of your model performances:\n",
    "\n",
    "Doc2Vec (Trained on both Train & Test Corpus) → 94.67%\n",
    "Doc2Vec (Train-only, Unseen Test Corpus) → 98.22%\n",
    "Word2Vec (Averaged Word Vectors) → 98.89%\n",
    "\n",
    "# Observations & Insights\n",
    "✅ Word2Vec performed best with 98.89% accuracy, showing that averaging word vectors effectively captures document meaning.\n",
    "✅ Doc2Vec (Train-only) improvConclusion:\n",
    "The Word2Vec model (Task 6) achieves the highest classification accuracy, likely because averaging word vectors provides a more robust document representation than Doc2Vec’s inferred vectorsed compared to full corpus training, indicating that generalization worked better when the test data was entirely unseen.\n",
    "✅ Both Doc2Vec approaches performed well, but training on the full corpus slightly reduced accuracy—possibly due to overfitting.\n",
    "\n",
    "Conclusion:\n",
    "The Word2Vec model (Task 6) achieves the highest classification accuracy, likely because averaging word vectors provides a more robust document representation than Doc2Vec’s inferred vectors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
