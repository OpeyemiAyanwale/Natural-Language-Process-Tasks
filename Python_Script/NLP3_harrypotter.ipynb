{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise sheet 3 with he file Potter.zip.  ”Harry Potter”-books. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "# In Moodle you will find the file Potter.zip. Unpack it. It contains 7 txt-files, each containing the text one of the ”Harry Potter”-books. Load those txt-files into your console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1187250, 1059022, 531708, 474429, 1608763, 676978, 1227024]\n"
     ]
    }
   ],
   "source": [
    "# option 1\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the path to the zip file and extraction directory\n",
    "zip_file_path = '/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/Potter.zip'\n",
    "extraction_dir = '/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/Potter'\n",
    "\n",
    "# Unpack the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extraction_dir)\n",
    "\n",
    "# Load all txt files into a list\n",
    "book_texts = []\n",
    "for filename in os.listdir(extraction_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(extraction_dir, filename), 'r', encoding='utf-8') as file:\n",
    "            book_texts.append(file.read())\n",
    "\n",
    "# Print loaded book texts (or their lengths for brevity)\n",
    "print([len(text) for text in book_texts])  # Output lengths of each book's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 books.\n"
     ]
    }
   ],
   "source": [
    "# option 2\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "# Define paths\n",
    "data_folder = \"/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/\"\n",
    "zip_path = os.path.join(data_folder, \"Potter.zip\")\n",
    "extract_folder = os.path.join(data_folder, \"PotterBooks\")\n",
    "\n",
    "# Unzip Potter.zip\n",
    "if not os.path.exists(extract_folder):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_folder)\n",
    "\n",
    "# Load the text files\n",
    "book_files = glob.glob(os.path.join(extract_folder, \"*.txt\"))\n",
    "\n",
    "# Read books into a dictionary\n",
    "books = {}\n",
    "for file_path in book_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        books[os.path.basename(file_path)] = f.readlines()\n",
    "\n",
    "print(f\"Loaded {len(books)} books.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "# To compare the books, we must know, which book it is we are looking at. Each file contains one particular line for every page in the book:\n",
    "# Page | page number book name - J.K. Rowling\n",
    "# Use regular expressions to automatically detect the name of the book from the texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Book Names Dictionary: {'file2.txt': 'Harry Potter and the Goblet of Fire', 'file3.txt': 'Page | 2 Harry Potter and the Half Blood Prince', 'file1.txt': 'Page | 2 Harry Potter and the Chamber of Secrets', 'file4.txt': 'Page | 2 Harry Potter and the Philosophers Stone', 'file5.txt': 'Page | 2 Harry Potter and the Order of the Phoenix', 'file7.txt': 'Page | 2 Harry Potter and the Prisoner of Azkaban', 'file6.txt': 'Page | 2 Harry Potter and the Deathly Hallows'}\n"
     ]
    }
   ],
   "source": [
    "#option 1\n",
    "\n",
    "import re\n",
    "\n",
    "book_names_dict = {}\n",
    "for filename, text in zip(os.listdir(extraction_dir), book_texts):\n",
    "    match = re.search(r\"(.+)- J\\.K\\. Rowling\", text)\n",
    "    if match:\n",
    "        # Strip leading/trailing whitespace and add to dictionary\n",
    "        book_name = match.group(1).strip()\n",
    "        book_names_dict[filename] = book_name\n",
    "\n",
    "print(\"Detected Book Names Dictionary:\", book_names_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Book Names Dictionary: {'file2.txt': 'Harry Potter and the Goblet of Fire', 'file3.txt': 'Harry Potter and the Half Blood Prince', 'file1.txt': 'Harry Potter and the Chamber of Secrets', 'file4.txt': 'Harry Potter and the Philosophers Stone', 'file5.txt': 'Harry Potter and the Order of the Phoenix', 'file7.txt': 'Harry Potter and the Prisoner of Azkaban', 'file6.txt': 'Harry Potter and the Deathly Hallows'}\n"
     ]
    }
   ],
   "source": [
    "# option 1 adjusted\n",
    "import re\n",
    "\n",
    "book_names_dict = {}\n",
    "for filename, text in zip(os.listdir(extraction_dir), book_texts):\n",
    "    # Adjust regex to capture clean book names without using look-behind\n",
    "    match = re.search(r'Page \\| \\d+ (.+?) - J\\.K\\. Rowling', text)\n",
    "    if match:\n",
    "        # Strip leading/trailing whitespace and add to dictionary\n",
    "        book_name = match.group(1).strip()\n",
    "        book_names_dict[filename] = book_name\n",
    "\n",
    "print(\"Detected Book Names Dictionary:\", book_names_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file2.txt': 'Harry Potter and the Goblet of Fire', 'file3.txt': 'Page | 2 Harry Potter and the Half Blood Prince', 'file1.txt': 'Page | 2 Harry Potter and the Chamber of Secrets', 'file4.txt': 'Page | 2 Harry Potter and the Philosophers Stone', 'file5.txt': 'Page | 2 Harry Potter and the Order of the Phoenix', 'file7.txt': 'Page | 2 Harry Potter and the Prisoner of Azkaban', 'file6.txt': 'Page | 2 Harry Potter and the Deathly Hallows'}\n"
     ]
    }
   ],
   "source": [
    "# option 2 but better to use option 1 adjusted\n",
    "import re\n",
    "\n",
    "def extract_book_title(text_lines):\n",
    "    for line in text_lines:\n",
    "        match = re.search(r\"(.+)- J\\.K\\. Rowling\", line)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    return \"Unknown Title\"\n",
    "\n",
    "book_titles = {file: extract_book_title(content) for file, content in books.items()}\n",
    "print(book_titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 3\n",
    "# The texts in the the txt-files are not ”clean” yet. To analyze them properly, we need to do additional preprocessing steps.\n",
    "# • Remove the page indicator from the texts. That is, remove all lines that have the form mentioned in task 2\n",
    "# • Trim the start of the document until the first chapter starts.\n",
    "# • Remove the headers of all chapters. These are written in CAPS (all letters are capitalized).\n",
    "# Detect this using regular expressions.\n",
    "# • Replace all line breaks (”\\n”) with a whitespace (” ”).\n",
    "# The result should be a list of 7 large stings, one for each book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Books Lengths: [1145446, 997862, 497716, 443794, 1578245, 632635, 1157662]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_book_text(text):\n",
    "    # Step 1: Remove page indicators using regex\n",
    "    cleaned_text = re.sub(r'Page \\| \\d+ .+? - J\\.K\\. Rowling\\n?', '', text)\n",
    "\n",
    "    ## Step 2: Trim until first chapter starts (assuming chapters start with \"CHAPTER\")\n",
    "    cleaned_text = cleaned_text.split('CHAPTER', 1)[-1] if 'CHAPTER' in cleaned_text else cleaned_text\n",
    "\n",
    "    # Step 3: Remove chapter headers (all caps)\n",
    "    cleaned_text = re.sub(r'\\n[A-Z\\s]+\\n?', ' ', cleaned_text)\n",
    "\n",
    "    # Step 4: Replace line breaks with whitespace\n",
    "    cleaned_text = re.sub(r'\\n+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Clean all books\n",
    "cleaned_books = [clean_book_text(text) for text in book_texts]\n",
    "\n",
    "# Print lengths of cleaned books to verify processing\n",
    "print(\"Cleaned Books Lengths:\", [len(book) for book in cleaned_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_book_text(text_lines):\n",
    "    # Remove page indicators\n",
    "    text_lines = [line for line in text_lines if not re.match(r\"Page \\| \\d+\", line)]\n",
    "    \n",
    "    # Find where the first chapter starts\n",
    "    start_idx = next((i for i, line in enumerate(text_lines) if \"CHAPTER\" in line.upper()), 0)\n",
    "    text_lines = text_lines[start_idx:]\n",
    "\n",
    "    # Remove chapter headers (all caps words)\n",
    "    text_lines = [line for line in text_lines if not re.fullmatch(r\"[A-Z\\s]+\", line.strip())]\n",
    "\n",
    "    # Merge lines into a single string\n",
    "    return \" \".join(line.strip() for line in text_lines if line.strip())\n",
    "\n",
    "cleaned_books = {title: clean_book_text(content) for title, content in books.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "# Apply elementary preprocessing steps in any order you prefer: punctuation- and number re- moval, lower casing, lemmatization/stemming and stop word removal. The result should be a list of lists. Each inner list represents a book as a list of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Books Sample: [['villager', 'little', 'hangleton', 'still', 'called', 'iddle', 'house', 'even', 'though', 'many'], ['j', 'nearing', 'midnight', 'prime', 'minister', 'sitting', 'alone', 'office', 'reading', 'long'], ['k', 'r', 'w', 'l', 'n', 'g', 'ot', 'first', 'time', 'argument'], ['r', 'mr', 'dursley', 'number', 'four', 'privet', 'drive', 'proud', 'say', 'perfectly'], ['harry', 'hottest', 'day', 'summer', 'far', 'drawing', 'close', 'drowsy', 'silence', 'lay'], ['arry', 'potter', 'highly', 'unusual', 'boy', 'many', 'way', 'one', 'thing', 'hated'], ['two', 'men', 'appeared', 'nowhere', 'yard', 'apart', 'narrow', 'moonlit', 'lane', 'second']]\n"
     ]
    }
   ],
   "source": [
    "# option 1\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources if not already done.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_book(book):\n",
    "    # Step 1: Tokenize the text into words, removing punctuation and numbers\n",
    "    words = re.findall(r'\\b\\w+\\b', book.lower())\n",
    "    \n",
    "    # Step 2: Lemmatization and stop word removal \n",
    "    processed_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return processed_words\n",
    "\n",
    "# Process all cleaned books\n",
    "processed_books = [preprocess_book(book) for book in cleaned_books]\n",
    "\n",
    "# Print a sample of processed books (first 10 words of each)\n",
    "print(\"Processed Books Sample:\", [processed[:10] for processed in processed_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Books Sample: [['villager', 'little', 'hangleton', 'still', 'called', 'iddle', 'house', 'even', 'though', 'many'], ['j', 'wa', 'nearing', 'midnight', 'prime', 'minister', 'wa', 'sitting', 'alone', 'office'], ['k', 'r', 'w', 'l', 'n', 'g', 'ot', 'first', 'time', 'argument'], ['dursley', 'number', 'four', 'privet', 'drive', 'proud', 'say', 'perfectly', 'normal', 'thank'], ['harry', 'hottest', 'day', 'summer', 'far', 'wa', 'drawing', 'close', 'drowsy', 'silence'], ['arry', 'potter', 'wa', 'highly', 'unusual', 'boy', 'many', 'way', 'one', 'thing'], ['two', 'men', 'appeared', 'nowhere', 'yard', 'apart', 'narrow', 'moonlit', 'lane', 'second']]\n"
     ]
    }
   ],
   "source": [
    "#option 1b\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Ensure necessary resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words and stemmer/lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation &; numbers, convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "    # Apply stemming (or lemmatization based on preference)\n",
    "    # Uncomment one of the following lines based on your choice:\n",
    "    # processed_words = [stemmer.stem(word) for word in words]\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Remove stopwords\n",
    "    processed_words = [word for word in processed_words if word not in stop_words]\n",
    "\n",
    "    return processed_words\n",
    "\n",
    "# Process all cleaned books using the new preprocessing function\n",
    "processed_books = [preprocess_text(book) for book in cleaned_books]\n",
    "\n",
    "# Print a sample of processed books (first 10 words of each)\n",
    "print(\"Processed Books Sample:\", [processed[:10] for processed in processed_books])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Each Step:\n",
    "Tokenization: We use a regex pattern (\\b\\w+\\b) to find all word-like sequences while ignoring punctuation and numbers.\n",
    "Lowercasing: Converting everything to lowercase ensures uniformity when analyzing the text.\n",
    "Lemmatization: This reduces words to their base form (e.g., \"running\" becomes \"run\").\n",
    "Stop Word Removal: Common English stop words (like \"and\", \"the\", etc.) are filtered out to focus on more meaningful terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Books Sample: [['villager', 'little', 'hangleton', 'still', 'called', 'iddle', 'house', 'even', 'though', 'many'], ['j', 'wa', 'nearing', 'midnight', 'prime', 'minister', 'wa', 'sitting', 'alone', 'office'], ['k', 'r', 'w', 'l', 'n', 'g', 'ot', 'first', 'time', 'argument'], ['dursley', 'number', 'four', 'privet', 'drive', 'proud', 'say', 'perfectly', 'normal', 'thank'], ['harry', 'hottest', 'day', 'summer', 'far', 'wa', 'drawing', 'close', 'drowsy', 'silence'], ['arry', 'potter', 'wa', 'highly', 'unusual', 'boy', 'many', 'way', 'one', 'thing'], ['two', 'men', 'appeared', 'nowhere', 'yard', 'apart', 'narrow', 'moonlit', 'lane', 'second']]\n"
     ]
    }
   ],
   "source": [
    "# option 2 the best here \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation & numbers, convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "    # Apply stemming and lemmatization\n",
    "    #stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Remove stopwords\n",
    "    processed_words = [word for word in lemmatized_words if word not in stop_words]\n",
    "\n",
    "    return processed_words\n",
    "\n",
    "preprocessed_books = {title: preprocess_text(content) for title, content in cleaned_books.items()}\n",
    "\n",
    "# Print a sample of processed books (first 10 words of each)\n",
    "print(\"Processed Books Sample:\", [processed[:10] for processed in processed_books])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed text from 'file2.txt':\n",
      "villager little hangleton still called riddle house even though many year since riddle family lived stood hill overlooking village window boarded tile missing roof ivy spreading unchecked face manor easily largest grandest building mile around riddle house wa damp derelict unoccupied little hangletons agreed old house wa half century ago\n"
     ]
    }
   ],
   "source": [
    "# Print a sample from the first book (first 50 words)\n",
    "sample_book_title = list(preprocessed_books.keys())[0]  # Get the first book title\n",
    "sample_words = preprocessed_books[sample_book_title][:50]  # First 50 words\n",
    "\n",
    "print(f\"Sample processed text from '{sample_book_title}':\")\n",
    "print(\" \".join(sample_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some words like \"wa\" (past tense was) are not removed by default. Let's add custom stop words:\n",
    "\n",
    "custom_stop_words = set(stop_words).union({\"wa\", \"ha\", \"said\"})  # Add more as needed\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return [word for word in lemmatized_words if word not in custom_stop_words]\n",
    "\n",
    "preprocessed_books = {title: preprocess_text(content) for title, content in cleaned_books.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved sample processed text from 'file2.txt':\n",
      "villager little hangleton still called riddle house even though many year since riddle family lived stood hill overlooking village window boarded tile missing roof ivy spreading unchecked face manor easily largest grandest building mile around riddle house damp derelict unoccupied little hangletons agreed old house half century ago something strange\n"
     ]
    }
   ],
   "source": [
    "sample_book_title = list(preprocessed_books.keys())[0]\n",
    "sample_words = preprocessed_books[sample_book_title][:50]\n",
    "\n",
    "print(f\"Improved sample processed text from '{sample_book_title}':\")\n",
    "print(\" \".join(sample_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "# Calculate the tfidf for your corpus. Return the words with the highest tfidf for each of the 7 books. # Does the result give you an idea of what the books are about? If not, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 TF-IDF Words for file2.txt:\n",
      "harry: 0.6935\n",
      "ron: 0.2017\n",
      "hermione: 0.1680\n",
      "potter: 0.1268\n",
      "rowling: 0.1215\n",
      "dumbledore: 0.1150\n",
      "back: 0.1144\n",
      "fire: 0.1065\n",
      "bagman: 0.1044\n",
      "one: 0.0976\n",
      "\n",
      "Top 10 TF-IDF Words for file3.txt:\n",
      "harry: 0.6782\n",
      "dumbledore: 0.2788\n",
      "slughorn: 0.1940\n",
      "ron: 0.1793\n",
      "hermione: 0.1259\n",
      "could: 0.1223\n",
      "would: 0.1212\n",
      "snape: 0.1021\n",
      "malfoy: 0.0984\n",
      "back: 0.0953\n",
      "\n",
      "Top 10 TF-IDF Words for file1.txt:\n",
      "harry: 0.6898\n",
      "ron: 0.2973\n",
      "hermione: 0.1597\n",
      "lockhart: 0.1541\n",
      "riddle: 0.1182\n",
      "back: 0.1037\n",
      "one: 0.1019\n",
      "malfoy: 0.0971\n",
      "could: 0.0929\n",
      "professor: 0.0894\n",
      "\n",
      "Top 10 TF-IDF Words for file4.txt:\n",
      "harry: 0.6642\n",
      "ron: 0.2140\n",
      "hagrid: 0.1863\n",
      "hermione: 0.1345\n",
      "one: 0.1314\n",
      "back: 0.1299\n",
      "got: 0.1037\n",
      "get: 0.1022\n",
      "know: 0.1017\n",
      "could: 0.0997\n",
      "\n",
      "Top 10 TF-IDF Words for file5.txt:\n",
      "harry: 0.6414\n",
      "hermione: 0.2363\n",
      "ron: 0.2105\n",
      "umbridge: 0.1984\n",
      "back: 0.1323\n",
      "professor: 0.1148\n",
      "well: 0.1141\n",
      "sirius: 0.1092\n",
      "could: 0.1043\n",
      "dumbledore: 0.1010\n",
      "\n",
      "Top 10 TF-IDF Words for file7.txt:\n",
      "harry: 0.6595\n",
      "ron: 0.2659\n",
      "hermione: 0.2443\n",
      "lupin: 0.1990\n",
      "black: 0.1483\n",
      "professor: 0.1315\n",
      "back: 0.1205\n",
      "snape: 0.0984\n",
      "one: 0.0883\n",
      "hagrid: 0.0883\n",
      "\n",
      "Top 10 TF-IDF Words for file6.txt:\n",
      "harry: 0.6630\n",
      "hermione: 0.2564\n",
      "ron: 0.2477\n",
      "could: 0.1348\n",
      "wand: 0.1313\n",
      "dumbledore: 0.1154\n",
      "back: 0.1112\n",
      "know: 0.1004\n",
      "like: 0.0960\n",
      "one: 0.0930\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine processed words into a single string per book\n",
    "combined_books = {title: \" \".join(content) for title, content in preprocessed_books.items()}\n",
    "\n",
    "# Create a TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the combined text data\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_books.values())\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Extract top n terms with highest tf-idf score for each book\n",
    "top_n = 10  # Define how many top terms you want to retrieve\n",
    "tfidf_results = {}\n",
    "\n",
    "for i, title in enumerate(combined_books.keys()):\n",
    "    # Get sorted indices of tf-idf scores in descending order\n",
    "    sorted_indices = tfidf_matrix[i].toarray().flatten().argsort()[::-1]\n",
    "    \n",
    "    # Retrieve top n terms and their corresponding scores\n",
    "    top_terms_with_scores = [(feature_names[idx], tfidf_matrix[i, idx]) \n",
    "                             for idx in sorted_indices[:top_n]]\n",
    "    \n",
    "    # Store results in dictionary\n",
    "    tfidf_results[title] = top_terms_with_scores\n",
    "\n",
    "# Print out top n terms with their TF-IDF scores for each book\n",
    "for title, terms in tfidf_results.items():\n",
    "    print(f\"\\nTop {top_n} TF-IDF Words for {title}:\")\n",
    "    for term, score in terms:\n",
    "        print(f\"{term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Great! The TF-IDF results obtained provide a lot of insight into the content of each book. Here's a summary of what you can interpret from these results:\n",
    "\n",
    "### Observations:\n",
    "1. **Frequent Character Names**: \n",
    "   - **Harry, Ron, Hermione**: These three characters consistently appear among the top terms across all books, indicating their central roles in the story.\n",
    "   - Other notable characters like **Dumbledore**, **Snape**, and **Hagrid** also appear frequently, which aligns with their importance in the series.\n",
    "\n",
    "2. **Contextual Words**:\n",
    "   - Words like **\"back\"**, **\"could\"**, and **\"one\"** are common across multiple books but may not carry specific thematic weight on their own.\n",
    "   - Specific terms related to events or elements in the story (e.g., **\"umbridge,\" \"lockhart,\" \"lupin,\" \"bagman,\"**) suggest key plot points or themes relevant to those particular books.\n",
    "\n",
    "3. **Unique Terms per Book**: \n",
    "   - Each book has unique words that reflect its specific plot elements or character arcs (e.g., \"umbridge\" in file5.txt indicates her significance in that particular book).\n",
    "\n",
    "### Analysis of Content:\n",
    "- The presence of character names as high-TFIDF terms suggests that these books focus heavily on interpersonal relationships and character development.\n",
    "- If certain expected themes or important concepts are missing from your results, consider whether stop words or lemmatization may have inadvertently filtered them out.\n",
    "\n",
    "### Conclusion:\n",
    "The TF-IDF analysis gives you a good idea of what each book is about by highlighting significant characters and plot-related terms. \n",
    "It shows how much emphasis is placed on certain characters throughout the series and can help identify key themes present in each installment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top TF-IDF words for 'file2.txt':\n",
      "harry, ron, hermione, potter, rowling, dumbledore, back, fire, bagman, one\n",
      "\n",
      "Top TF-IDF words for 'file3.txt':\n",
      "harry, dumbledore, slughorn, ron, hermione, could, would, snape, malfoy, back\n",
      "\n",
      "Top TF-IDF words for 'file1.txt':\n",
      "harry, ron, hermione, lockhart, riddle, back, one, malfoy, could, professor\n",
      "\n",
      "Top TF-IDF words for 'file4.txt':\n",
      "harry, ron, hagrid, hermione, one, back, got, get, know, could\n",
      "\n",
      "Top TF-IDF words for 'file5.txt':\n",
      "harry, hermione, ron, umbridge, back, professor, well, sirius, could, dumbledore\n",
      "\n",
      "Top TF-IDF words for 'file7.txt':\n",
      "harry, ron, hermione, lupin, black, professor, back, snape, one, hagrid\n",
      "\n",
      "Top TF-IDF words for 'file6.txt':\n",
      "harry, hermione, ron, could, wand, dumbledore, back, know, like, one\n"
     ]
    }
   ],
   "source": [
    "# option 2\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert each book into a full string for TF-IDF processing\n",
    "book_texts = [\" \".join(words) for words in preprocessed_books.values()]\n",
    "\n",
    "# Apply TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 most important words\n",
    "tfidf_matrix = vectorizer.fit_transform(book_texts)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Function to get the highest TF-IDF words per book\n",
    "def get_top_tfidf_words(tfidf_matrix, feature_names, top_n=10):\n",
    "    top_words = {}\n",
    "    for book_idx, title in enumerate(preprocessed_books.keys()):\n",
    "        row = tfidf_matrix[book_idx].toarray()[0]\n",
    "        top_indices = row.argsort()[-top_n:][::-1]  # Get top N words with highest TF-IDF\n",
    "        top_words[title] = [feature_names[i] for i in top_indices]\n",
    "    return top_words\n",
    "\n",
    "# Get top TF-IDF words\n",
    "top_tfidf_words = get_top_tfidf_words(tfidf_matrix, feature_names)\n",
    "\n",
    "# Print results\n",
    "for book, words in top_tfidf_words.items():\n",
    "    print(f\"\\nTop TF-IDF words for '{book}':\")\n",
    "    print(\", \".join(words))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
