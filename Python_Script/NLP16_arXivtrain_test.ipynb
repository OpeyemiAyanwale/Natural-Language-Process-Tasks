{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb9c77a-2bec-47b2-ad27-36cf7a837294",
   "metadata": {},
   "source": [
    "# Sheet 11: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4909f4-db31-4f08-8d9f-b505f3d2ae2b",
   "metadata": {},
   "source": [
    "In moodle you can find the file arXiv train.csv and arXiv test.csv. It contains the titles,\n",
    "abstracts and categories of 107,055 papers uploaded to the preprint-platform arXiv. You are\n",
    "tasked to create a pipeline, which predicts the category (given in the terms column) of a paper.\n",
    "The train corpus contains 100 labeled examples for the 4 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3833f8-ddf5-498a-813d-7286b1589ff5",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "# Argue the usefulness of the following models for the task at hand in 2-3 sentences:\n",
    "• Dictionary-based analysis\n",
    "• Latent Dirichlet Allocation\n",
    "• Word2Vec\n",
    "• Doc2Vec\n",
    "• BERT\n",
    "# Are there other analyses that might help to solve this task? Which analyses do you deem tobe the best for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb3a85",
   "metadata": {},
   "source": [
    "# Task 1 \n",
    "Argue the usefulness of the following models for the task at hand in 2-3 sentences:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07dc5f66-7262-4017-8238-be434d82b71d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Evaluating Different Models\n",
    "\n",
    "Dictionary-based analysis\n",
    "This approach relies on predefined word lists for classification, making it ineffective for this task due to limited adaptability to unseen words or phrases.\n",
    "Given the complexity of scientific language and category overlap, a dictionary-based approach lacks generalization and context awareness.\n",
    "\n",
    "\n",
    "Latent Dirichlet Allocation (LDA)\n",
    "LDA is useful for topic modeling but not ideal for classification since it assigns topics rather than specific categories.\n",
    "It assumes documents contain multiple topics, which may not align well with single-label classification.\n",
    "\n",
    "\n",
    "Word2Vec\n",
    "Word2Vec captures word semantics through embeddings but lacks document-level context.\n",
    "While it can improve feature representation, it needs additional mechanisms like averaging or pooling for classification tasks.\n",
    "\n",
    "\n",
    "Doc2Vec\n",
    "Unlike Word2Vec, Doc2Vec creates embeddings for entire documents, making it more suitable for this task.\n",
    "However, it requires large training data to learn meaningful representations, which may be a limitation given the dataset’s size.\n",
    "\n",
    "\n",
    "BERT\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a strong candidate due to its contextual word understanding.\n",
    "It can capture deep semantic meanings, making it highly effective for text classification, especially in scientific abstracts.\n",
    "\n",
    "\n",
    "Best Model Choice:\n",
    "BERT (or a fine-tuned transformer model like SciBERT) would likely perform best due to its ability to handle contextual and domain-specific text.\n",
    "Alternative deep-learning models like TF-IDF + Logistic Regression or LSTM-based classifiers can also be effective.\n",
    "\n",
    "\n",
    "# Summary Argument for Model Usefulness\n",
    "Dictionary-based analysis: \"Limited usefulness due to lack of contextual understanding. Good for keyword-based classification but not suitable for nuanced category prediction.\",\n",
    "Latent Dirichlet Allocation\": \"Better for topic modeling, but not optimal for supervised classification tasks since LDA does not explicitly learn categories.\",\n",
    "Word2Vec: \"Useful for capturing word semantics but lacks document-level understanding.\",\n",
    "Doc2Vec: \"Improves on Word2Vec by representing entire documents, but requires large training data to generalize well.\",\n",
    "BERT: \"Best choice due to its contextualized embeddings and pre-trained knowledge, making it effective for text classification even with limited labeled data.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd3f92-bb4a-4d38-b675-1c49efc6df1b",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Preprocess the data so that it is fit for the pipeline you intend to solve the task with. Shortly\n",
    "explain every step of preprocessing and why it is useful for this analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d1701f0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "pip install pandas scikit-learn transformers torch nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b08c7f-12eb-44f0-9400-76521be1897f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oayanwale/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4dc872-e8ba-486a-a47e-c7db5f4ef80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              titles  \\\n",
      "0  Nearly Minimax Optimal Reinforcement Learning ...   \n",
      "1  An Intrinsically-Motivated Approach for Learni...   \n",
      "2  Adversarial Active Exploration for Inverse Dyn...   \n",
      "3                 Accelerated Reinforcement Learning   \n",
      "4  Learning Adaptive Display Exposure for Real-Ti...   \n",
      "\n",
      "                                           abstracts terms  \n",
      "0  We study reinforcement learning (RL) with line...  math  \n",
      "1  What is a good exploration strategy for an age...  stat  \n",
      "2  We present an adversarial active exploration f...  stat  \n",
      "3  Policy gradient methods are widely used in rei...    cs  \n",
      "4  In E-commerce advertising, where product recom...  stat  \n",
      "                                              titles  \\\n",
      "0  Change your singer: a transfer learning genera...   \n",
      "1  Towards Understanding Pixel Vulnerability unde...   \n",
      "2  Pedestrian Detection with Spatially Pooled Fea...   \n",
      "3  Relative concentration bounds for the spectrum...   \n",
      "4  Enhancing Trajectory Prediction using Sparse O...   \n",
      "\n",
      "                                           abstracts terms  \n",
      "0  Have you ever wondered how a song might sound ...  eess  \n",
      "1  Deep neural network image classifiers are repo...    cs  \n",
      "2  Many typical applications of object detection ...    cs  \n",
      "3  In this paper we study the concentration prope...  stat  \n",
      "4  Sophisticated trajectory prediction models tha...  stat  \n",
      "Train Data Columns: Index(['titles', 'abstracts', 'terms'], dtype='object')\n",
      "Test Data Columns: Index(['titles', 'abstracts', 'terms'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv(\"/Users/oayanwale/Downloads/NLP_Exercise_23/arxiv_train.csv\")\n",
    "test_data = pd.read_csv(\"/Users/oayanwale/Downloads/NLP_Exercise_23/arxiv_test.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "# Print the columns to check their names\n",
    "print(\"Train Data Columns:\", train_data.columns)\n",
    "print(\"Test Data Columns:\", test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa62fb8a-7178-4868-9318-77b70cd9c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 80\n",
      "Validation samples: 20\n",
      "                                                 text terms\n",
      "55  accelerating deep unsupervised domain adaptati...    cs\n",
      "88  improved part segmentation performance optimis...    cs\n",
      "26  attentive relational network mapping image sce...    cs\n",
      "42  transfer learning aided target recognition com...    cs\n",
      "69  whereandwhen look deep siamese attention netwo...    cs\n"
     ]
    }
   ],
   "source": [
    "#option 1 the best\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize text into words\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', word) for word in tokens]  # Remove special characters and numbers\n",
    "    tokens = [word for word in tokens if word and word not in stopwords.words('english')]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Apply lemmatization\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"/Users/oayanwale/Downloads/NLP_Exercise_23/arxiv_train.csv\")\n",
    "test_data = pd.read_csv(\"/Users/oayanwale/Downloads/NLP_Exercise_23/arxiv_test.csv\")\n",
    "\n",
    "# Apply preprocessing to titles and abstracts\n",
    "train_data['processed_titles'] = train_data['titles'].apply(preprocess_text)\n",
    "train_data['processed_abstracts'] = train_data['abstracts'].apply(preprocess_text)\n",
    "\n",
    "test_data['processed_titles'] = test_data['titles'].apply(preprocess_text)\n",
    "test_data['processed_abstracts'] = test_data['abstracts'].apply(preprocess_text)\n",
    "\n",
    "# Combine processed titles and abstracts into a single text column\n",
    "train_data['text'] = train_data['processed_titles'] + \" \" + train_data['processed_abstracts']\n",
    "test_data['text'] = test_data['processed_titles'] + \" \" + test_data['processed_abstracts']\n",
    "\n",
    "# Split training data into training and validation sets (80-20 split)\n",
    "train_set, val_set = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print dataset info\n",
    "print(\"Training samples:\", len(train_set))\n",
    "print(\"Validation samples:\", len(val_set))\n",
    "\n",
    "# Print first few processed rows\n",
    "print(train_set[['text', 'terms']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7706a9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 80\n",
      "Validation samples: 20\n",
      "                                                 text terms\n",
      "55  acceler deep unsupervis domain adapt transfer ...    cs\n",
      "88  improv part segment perform optimis realism sy...    cs\n",
      "26  attent relat network map imag scene graph scen...    cs\n",
      "42  transfer learn aid target recognit compar deep...    cs\n",
      "69  whereandwhen look deep siames attent network v...    cs\n"
     ]
    }
   ],
   "source": [
    "# option 2 \n",
    "\n",
    "# Preprocessing to prepare for furtherb analysis(raw datan into clean data)\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing for uniformity and consistency\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers to reduce noise\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    # Tokenization to split into individual word or tokens\n",
    "    tokens = word_tokenize(text) \n",
    "    # Stopword removal to remove irrelavant words and reduce noise\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization reduce word to their roof form\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Stemming to reduce word to their roof form\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to titles and abstracts separately using correct column names\n",
    "train_data['processed_titles'] = train_data['titles'].apply(preprocess_text)\n",
    "train_data['processed_abstracts'] = train_data['abstracts'].apply(preprocess_text)\n",
    "\n",
    "test_data['processed_titles'] = test_data['titles'].apply(preprocess_text)\n",
    "test_data['processed_abstracts'] = test_data['abstracts'].apply(preprocess_text)\n",
    "\n",
    "# Combine processed titles and abstracts for modeling purposes:\n",
    "train_data['text'] = train_data['processed_titles'] + \" \" + train_data['processed_abstracts']\n",
    "test_data['text'] = test_data['processed_titles'] + \" \" + test_data['processed_abstracts']\n",
    "\n",
    "# Split into training and validation sets (80-20 split) from training data only\n",
    "train_set, val_set = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training samples:\", len(train_set))\n",
    "print(\"Validation samples:\", len(val_set))\n",
    "\n",
    "# Optional: Print first few rows of processed data for verification \n",
    "print(train_set[['text', 'terms']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93590050",
   "metadata": {},
   "source": [
    "# Shortly explain every step of preprocessing and why it is useful for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad0203",
   "metadata": {},
   "source": [
    "Lowercasing: This step converts all characters in the text to lowercase. It ensures uniformity and consistency, preventing issues where the same word in different cases (e.g., \"Deep\" vs. \"deep\") is treated as distinct tokens during analysis.\n",
    "\n",
    "Removing Special Characters and Numbers: By removing special characters and numbers, we reduce noise in the data that may not contribute meaningfully to understanding the content. This helps focus on relevant words that carry semantic weight\n",
    "\n",
    "\n",
    "Tokenization: Tokenization splits the cleaned text into individual words or tokens. This is essential for further processing steps like stopword removal, stemming, or lemmatization, as it allows you to operate on each word individually.\n",
    "\n",
    "Stopword Removal: Stopwords are common words (e.g., \"and,\" \"the,\" \"is\") that typically do not add significant meaning to texts and can be safely removed to reduce noise. This helps improve model performance by focusing on more meaningful terms related to classification tasks.\n",
    "\n",
    "Lemmatization: Lemmatization reduces words to their base or root form (e.g., \"running\" becomes \"run\"). Unlike stemming, which may produce non-words (e.g., \"better\" → \"better\"), lemmatization maintains valid dictionary forms. This step enhances semantic understanding by grouping different forms of a word together.\n",
    "\n",
    "Stemming: Stemming further reduces words to their root form but does so more aggressively than lemmatization (e.g., “running” → “run,” “better” → “better”). While stemming can sometimes lead to less accurate representations, it’s useful when you want a more generalized representation of terms across documents.\n",
    "\n",
    "\n",
    "Combining Processed Texts: After processing titles and abstracts separately, they are combined into one string to creates a single textual representation that captures all relevant information about each paper, making it easier for models to learn from both components simultaneously during training.\n",
    "\n",
    "\n",
    "Splitting Data into Training and Validation Sets: Splitting the dataset allows us to evaluate model performance on unseen data after training without overfitting on the entire dataset. The validation set serves as a benchmark to assess how well your model generalizes beyond its training examples.\n",
    "\n",
    "\n",
    "Summary of Importance:\n",
    "✅ Lowercasing: Standardizes text and avoids case-related discrepancies.\n",
    "✅ Removing special characters and numbers: Helps in reducing noise.\n",
    "✅ Tokenization: Splitting text into individual words improves analysis.\n",
    "✅ Stopword removal: Eliminates common but non-informative words.\n",
    "✅ Lemmatization & Stemming: Helps to reduce words to their base/root form, improving model generalization.\n",
    "✅ Combining titles & abstracts: Creates a richer representation of the research paper for classification.\n",
    "✅ Training-validation split: Ensures model evaluation is done on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1770b2-bb02-4474-812e-cc551e248700",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "Solve the task by creating a pipeline with the model you deem to be optimal for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df13dcab-540b-4c53-aee1-daf7c7724675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "407d2da4-76db-4704-817d-3ebc68a73f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 26s 3s/step - loss: 1.2112 - accuracy: 0.4300\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 18s 3s/step - loss: 0.9822 - accuracy: 0.6400\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 17s 2s/step - loss: 0.9051 - accuracy: 0.6100\n",
      "6685/6685 [==============================] - 5769s 863ms/step\n"
     ]
    }
   ],
   "source": [
    "#option 1\n",
    "\n",
    "# BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['label'] = label_encoder.fit_transform(train_data['terms'])\n",
    "\n",
    "# Tokenization and padding\n",
    "max_length = 128\n",
    "train_encodings = tokenizer(list(train_data['titles'] + ' ' + train_data['abstracts']), truncation=True, padding=True, max_length=max_length)\n",
    "test_encodings = tokenizer(list(test_data['titles'] + ' ' + test_data['abstracts']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert to TensorFlow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_data['label'].values\n",
    "))\n",
    "\n",
    "# BERT Model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Training\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)\n",
    "\n",
    "# Prediction\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    np.zeros(len(test_data))  # Dummy labels, as they are not used during prediction\n",
    "))\n",
    "predictions = model.predict(test_dataset.batch(16))\n",
    "predicted_labels = np.argmax(predictions.logits, axis=1)\n",
    "predicted_categories = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Add predicted categories to test_data\n",
    "test_data['predicted_category'] = predicted_categories \n",
    "\n",
    "# Save results to CSV\n",
    "test_data.to_csv(\"arXiv_test_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfe03dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7a7812e8ca48d8a2cc82bae02e6a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 45.3315, 'train_samples_per_second': 5.294, 'train_steps_per_second': 0.662, 'train_loss': 1.4401972452799479, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c37d6bb7c64064a03ad5776fbc215c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3773667812347412,\n",
       " 'eval_runtime': 1.3815,\n",
       " 'eval_samples_per_second': 14.477,\n",
       " 'eval_steps_per_second': 2.172,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option 2\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load BERT tokenizer and model \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(train_set['terms'].unique()))  # Adjust num_labels based on your classes\n",
    "\n",
    "# Create a label mapping\n",
    "label_mapping = {label: idx for idx, label in enumerate(train_set['terms'].unique())}\n",
    "\n",
    "# Tokenize input texts for BERT model \n",
    "def tokenize_function(examples):\n",
    "    return tokenizer([example['text'] for example in examples], padding='max_length', truncation=True)\n",
    "\n",
    "# Prepare datasets for training and validation\n",
    "train_encodings = tokenize_function(train_set.to_dict(orient='records'))\n",
    "val_encodings = tokenize_function(val_set.to_dict(orient='records'))\n",
    "\n",
    "class ArxivDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        # Convert string labels to numerical values using label mapping\n",
    "        self.labels = [label_mapping[label] for label in labels]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create dataset instances with numerical labels\n",
    "train_dataset = ArxivDataset(train_encodings, train_set['terms'].values)  # Ensure 'terms' contains correct category labels\n",
    "val_dataset = ArxivDataset(val_encodings, val_set['terms'].values)\n",
    "\n",
    "# Set up training arguments \n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model \n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model \n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc49f69",
   "metadata": {},
   "source": [
    "Training Metrics\n",
    "Train Runtime: The total time taken for training (in seconds).\n",
    "Train Samples per Second: The number of samples processed per second during training.\n",
    "Train Steps per Second: The number of training steps completed per second.\n",
    "Train Loss: The average loss value during training; lower values indicate better performance as it means the model's predictions are closer to the actual labels.\n",
    "Epoch: Indicates that the training ran through all samples in your dataset for three complete passes (epochs).\n",
    "\n",
    "Evaluation Metrics\n",
    "Eval Loss: The average loss value calculated on your validation set after training; similar to train loss, lower values indicate better performance.\n",
    "Eval Runtime: Total time taken for evaluation (in seconds).\n",
    "Eval Samples per Second: Number of samples processed per second during evaluation.\n",
    "Eval Steps per Second: Number of evaluation steps completed per second.\n",
    "Epoch: Indicates that this evaluation occurred after completing three epochs.\n",
    "\n",
    "Summary\n",
    "Overall, these metrics suggest that our model has been trained successfully over three epochs with reasonable runtime and loss values for both training and evaluation phases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbaedd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/oayanwale/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cf9206e2be4964a8d823366d1af129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0429dd0b66644f886b13faeb8bb73e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.385432243347168, 'eval_runtime': 1.3122, 'eval_samples_per_second': 15.242, 'eval_steps_per_second': 2.286, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47094ac4eabf4fa9b707d64e4dbc81c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2857633829116821, 'eval_runtime': 0.8658, 'eval_samples_per_second': 23.1, 'eval_steps_per_second': 3.465, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3bb6fcda1142dc8993f2a5cef2c62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.242681622505188, 'eval_runtime': 0.8739, 'eval_samples_per_second': 22.887, 'eval_steps_per_second': 3.433, 'epoch': 3.0}\n",
      "{'train_runtime': 52.7304, 'train_samples_per_second': 4.551, 'train_steps_per_second': 0.569, 'train_loss': 1.2927488962809244, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78447777ff6449ad8c03e764ad25c709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.242681622505188,\n",
       " 'eval_runtime': 0.8897,\n",
       " 'eval_samples_per_second': 22.481,\n",
       " 'eval_steps_per_second': 3.372,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option 3 the best\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "num_labels = len(train_set['terms'].unique())  # Ensure num_labels match unique categories\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create a label mapping\n",
    "label_mapping = {label: idx for idx, label in enumerate(train_set['terms'].unique())}\n",
    "\n",
    "# Tokenize function for dataset\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize training and validation data\n",
    "train_encodings = tokenize_function(train_set['text'].tolist())\n",
    "val_encodings = tokenize_function(val_set['text'].tolist())\n",
    "\n",
    "# Custom PyTorch dataset class\n",
    "class ArxivDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor([label_mapping[label] for label in labels], dtype=torch.long)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = ArxivDataset(train_encodings, train_set['terms'].values)\n",
    "val_dataset = ArxivDataset(val_encodings, val_set['terms'].values)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",  # Ensure periodic evaluation\n",
    "    save_strategy=\"epoch\",  # Save at each epoch\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369eb5df",
   "metadata": {},
   "source": [
    "Loss is Decreasing:\n",
    "Epoch 1 → eval_loss: 1.3854\n",
    "Epoch 2 → eval_loss: 1.2857\n",
    "Epoch 3 → eval_loss: 1.2426\n",
    "So our model is learning! 🎉\n",
    "\n",
    "Training Speed\n",
    "Our model trained 30 steps in 52.7s, which is reasonable given BERT's complexity.\n",
    "Train Loss (1.2927) is close to Eval Loss (1.2426), suggesting no severe overfitting.\n",
    "\n",
    "Next Steps for Improvement:\n",
    "✅ Increase Epochs (if needed):\n",
    "The loss is still decreasing, so more epochs (e.g., num_train_epochs=5) might improve results.\n",
    "\n",
    "✅ Use Learning Rate Scheduling & Early Stopping:\n",
    "A smaller learning rate (learning_rate=5e-5) with early stopping can prevent overfitting.\n",
    "\n",
    "✅ Optimize Batch Size for Speed:\n",
    "If you have enough memory, increase per_device_train_batch_size=16 to speed up training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c66f806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15763bb23b2b41c59599ecd52811c1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.66824746,  0.13974105,  0.23510711, -0.70673525],\n",
       "       [ 0.62987137,  0.1361922 ,  0.22343136, -0.73542416],\n",
       "       [ 0.6403691 ,  0.17562026,  0.24784558, -0.6988257 ],\n",
       "       ...,\n",
       "       [ 0.60470325,  0.11864298,  0.22632928, -0.70553744],\n",
       "       [ 0.5936044 ,  0.13653181,  0.23116635, -0.7266377 ],\n",
       "       [ 0.65835637,  0.17463589,  0.22392042, -0.7050129 ]],\n",
       "      dtype=float32), label_ids=array([3, 0, 0, ..., 0, 0, 0]), metrics={'test_loss': 1.1664959192276, 'test_runtime': 4776.4558, 'test_samples_per_second': 22.392, 'test_steps_per_second': 2.799})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Model Performance on Test Data\n",
    "\n",
    "# Tokenize test data\n",
    "test_encodings = tokenize_function(test_data['text'].tolist())  # Ensure a list of strings is passed\n",
    "\n",
    "\n",
    "# Create test dataset instance\n",
    "test_dataset = ArxivDataset(test_encodings, test_data['terms'].values)\n",
    "\n",
    "# Evaluate model on test dataset\n",
    "trainer.predict(test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
