{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise sheet 8 with the  biden.csv tweet before election. Load the data set into your console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "# In moodle you will find the file biden.csv. It contains every tweet in the month prior to the U.S. presidential election in 2020 containing the hash tag #joebiden. \n",
    "# Load the file into your console. It contains each tweet in the “tweet” column and the date of the tweet’s creation in the “created at” column.\n",
    "\n",
    "# We are interested in how the topics of the tweets develop over time. For this, we will train a dynamic topic model called RollingLDA on the speeches and compare the resulting topical changes in the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet           created_at\n",
      "0  #ElectionNight #MSNBC2020 #IVoted #Biden2020 #...  2020-11-03 23:41:02\n",
      "1  Go Headlines: #TopNews Of The Hour\\n#USElectio...  2020-11-04 09:10:42\n",
      "2  I doubt the person(s) who stole our official B...  2020-10-18 13:25:15\n",
      "3  The Bidens are safe so long as Fox News is the...  2020-10-21 09:30:52\n",
      "4  Since I live in a republican state TIME FOR ME...  2020-11-07 17:58:18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/biden.csv'\n",
    "biden_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe and check relevant columns\n",
    "print(biden_df[['tweet', 'created_at']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           created_at                                              tweet\n",
      "0 2020-11-03 23:41:02  #ElectionNight #MSNBC2020 #IVoted #Biden2020 #...\n",
      "1 2020-11-04 09:10:42  Go Headlines: #TopNews Of The Hour\\n#USElectio...\n",
      "2 2020-10-18 13:25:15  I doubt the person(s) who stole our official B...\n",
      "3 2020-10-21 09:30:52  The Bidens are safe so long as Fox News is the...\n",
      "4 2020-11-07 17:58:18  Since I live in a republican state TIME FOR ME...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 129996 entries, 0 to 129995\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype         \n",
      "---  ------      --------------   -----         \n",
      " 0   created_at  129996 non-null  datetime64[ns]\n",
      " 1   tweet       129996 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(1)\n",
      "memory usage: 2.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# option 2 loading data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define file path\n",
    "data_folder = \"/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/\"\n",
    "biden_path = f\"{data_folder}/biden.csv\"\n",
    "\n",
    "# Load dataset\n",
    "biden_df = pd.read_csv(biden_path, parse_dates=[\"created_at\"])  # Ensure date is in correct format\n",
    "\n",
    "# Display basic info\n",
    "print(biden_df.head())\n",
    "print(biden_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "# Remove unwanted fragments that are not relevant for our analysis. \n",
    "# Preprocess the texts so that they are fit for an analysis. \n",
    "# Argue the use the preprocessing steps you take for the given analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0  #ElectionNight #MSNBC2020 #IVoted #Biden2020 #...   \n",
      "1  Go Headlines: #TopNews Of The Hour\\n#USElectio...   \n",
      "2  I doubt the person(s) who stole our official B...   \n",
      "3  The Bidens are safe so long as Fox News is the...   \n",
      "4  Since I live in a republican state TIME FOR ME...   \n",
      "\n",
      "                                     processed_tweet  \n",
      "0  electionnight msnbc ivoted biden bluewave vote...  \n",
      "1  go headline topnews hour uselection trump accu...  \n",
      "2  doubt person stole official biden harris sign ...  \n",
      "3  bidens safe long fox news news outlet reportin...  \n",
      "4  since live republican state time start harassi...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Ensure necessary resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers, keeping only alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize (split into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize remaining words\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Apply preprocessing to tweet texts column (assuming it's named 'tweet')\n",
    "biden_df['processed_tweet'] = biden_df['tweet'].apply(preprocess_text)\n",
    "\n",
    "# Display processed tweets for verification\n",
    "print(biden_df[['tweet', 'processed_tweet']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since we want meaningful topic modeling, we must preprocess:\n",
    "✅ Remove non-text fragments (RT @user, links, hashtags). \n",
    "{which are often not relevant for sentiment analysis or topic modeling since they don't contribute to the semantic meaning}.\n",
    "\n",
    "✅ Convert to lowercase. \n",
    "{ensures uniformity in word representation (e.g., \"Vote\" and \"vote\" are treated as the same word)}.\n",
    "\n",
    "✅ Remove punctuation, numbers, and special characters. \n",
    "{were effectively stripped out, leading to cleaner tokens that focus on meaningful words}.\n",
    "\n",
    "✅ Tokenize, remove stopwords, and lemmatize words. \n",
    "{The transformation into tokenized words allows for easier manipulation during analysis, especially when using models like LDA that require document-term matrices.}\n",
    "\n",
    "{Common stop words (like \"the\", \"is\", \"and\") were likely filtered out. This is crucial because such words do not add significant meaning to topics or sentiments being analyzed.}\n",
    "\n",
    "✅ Lemmatization: Reducing words to their base forms helps in consolidating different variations of a word (e.g., \"running\" becomes \"run\"), which is important for accurately capturing themes in topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0  #ElectionNight #MSNBC2020 #IVoted #Biden2020 #...   \n",
      "1  Go Headlines: #TopNews Of The Hour\\n#USElectio...   \n",
      "2  I doubt the person(s) who stole our official B...   \n",
      "3  The Bidens are safe so long as Fox News is the...   \n",
      "4  Since I live in a republican state TIME FOR ME...   \n",
      "\n",
      "                                     processed_tweet  \n",
      "0                                                 []  \n",
      "1  [go, headline, hour, accuses, campaign, fraud,...  \n",
      "2  [doubt, person, stole, official, bidenharris, ...  \n",
      "3  [bidens, safe, long, fox, news, news, outlet, ...  \n",
      "4  [since, live, republican, state, time, start, ...  \n"
     ]
    }
   ],
   "source": [
    "# option 2 \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure required downloads\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()  # Lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\S+|#\\S+\", \"\", text)  # Remove URLs, mentions, hashtags\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # Remove punctuation & numbers\n",
    "    words = word_tokenize(text)  # Tokenize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Remove stopwords & lemmatize\n",
    "    #return \" \".join(words)\n",
    "    return words  # RETURN LIST (not a string) for rollinglda\n",
    "\n",
    "# Apply preprocessing\n",
    "biden_df[\"processed_tweet\"] = biden_df[\"tweet\"].apply(preprocess_text)\n",
    "\n",
    "# Print sample\n",
    "print(biden_df[[\"tweet\", \"processed_tweet\"]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguing for These Preprocessing Steps\n",
    "When arguing for our preprocessing steps in an analysis context, consider emphasizing:\n",
    "\n",
    "Relevance to Objectives: Each step taken aligns with our goal of analyzing emotions or topics within tweets by focusing on meaningful content while removing noise.\n",
    "\n",
    "Model Efficiency: By reducing dimensionality through removing irrelevant fragments and stop words, we improve computational efficiency and model performance.\n",
    "\n",
    "Data Quality Improvement: Effective preprocessing enhances data quality by ensuring that only relevant information is retained for analysis, leading to more accurate insights.\n",
    "\n",
    "Conclusion\n",
    "Overall, our preprocessing steps appear well-suited for preparing tweet data for further analysis using techniques like LDA or sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "# Train a normal LDA on the entire corpus with K = 30.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, '0.261*\"#\" + 0.157*\"election\" + 0.055*\"#joebiden\" + 0.041*\"#biden\" + 0.023*\"#china\"')\n",
      "(7, '0.184*\"president\" + 0.071*\"#joebiden\" + 0.041*\"first\" + 0.038*\"u\" + 0.038*\"vice\"')\n",
      "(29, '0.093*\"good\" + 0.066*\"love\" + 0.054*\"call\" + 0.052*\"hope\" + 0.051*\"#joebiden\"')\n",
      "(19, '0.077*\"really\" + 0.050*\"@\" + 0.043*\"bye\" + 0.040*\"fact\" + 0.032*\"hate\"')\n",
      "(27, '0.270*\"amp\" + 0.039*\"black\" + 0.035*\"called\" + 0.023*\"number\" + 0.022*\"actually\"')\n",
      "(18, '0.078*\"news\" + 0.051*\"medium\" + 0.039*\"#biden\" + 0.033*\"war\" + 0.031*\"#byedon\"')\n",
      "(3, '0.052*\"big\" + 0.044*\"family\" + 0.043*\"become\" + 0.037*\"got\" + 0.036*\"around\"')\n",
      "(4, '0.073*\"elected\" + 0.071*\"#biden\" + 0.070*\"#vote\" + 0.053*\"#bidenharris\" + 0.047*\"winner\"')\n",
      "(26, '0.121*\"usa\" + 0.046*\"india\" + 0.037*\"#bidenharristoendthisnightmare\" + 0.032*\"never\" + 0.028*\"st\"')\n",
      "(16, '0.123*\"#uselection\" + 0.099*\"#joebiden\" + 0.077*\"#donaldtrump\" + 0.059*\"#usa\" + 0.058*\"#america\"')\n",
      "(0, '0.081*\"u\" + 0.065*\"election\" + 0.061*\"trump\" + 0.053*\"#biden\" + 0.053*\"#joebiden\"')\n",
      "(25, '0.183*\"result\" + 0.050*\"#trumpmeltdown\" + 0.047*\"electoral\" + 0.045*\"change\" + 0.035*\"#trumpislosing\"')\n",
      "(8, '0.180*\"win\" + 0.095*\"#biden\" + 0.042*\"biden\" + 0.037*\"#trump\" + 0.036*\"trump\"')\n",
      "(17, '0.054*\"#biden\" + 0.042*\"#joebiden\" + 0.034*\"people\" + 0.030*\"world\" + 0.022*\"america\"')\n",
      "(5, '0.095*\"#biden\" + 0.091*\"#harris\" + 0.079*\"#covid\" + 0.051*\"new\" + 0.045*\"#georgia\"')\n",
      "(10, '0.075*\"voted\" + 0.049*\"million\" + 0.042*\"voter\" + 0.041*\"yes\" + 0.041*\"count\"')\n",
      "(28, '0.209*\"#bidenharris\" + 0.137*\"#biden\" + 0.099*\"year\" + 0.028*\"old\" + 0.028*\"four\"')\n",
      "(23, '0.063*\"#pennsylvania\" + 0.052*\"#cnn\" + 0.037*\"#foxnews\" + 0.032*\"#arizona\" + 0.031*\"#michigan\"')\n",
      "(2, '0.086*\"#president\" + 0.082*\"thank\" + 0.064*\"god\" + 0.055*\"lost\" + 0.035*\"#breakingnews\"')\n",
      "(24, '0.061*\"lead\" + 0.054*\"finally\" + 0.050*\"#biden\" + 0.028*\"sure\" + 0.028*\"give\"')\n"
     ]
    }
   ],
   "source": [
    "# Train normal LDA using Gensim\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Convert processed tweets into lists of tokenized words.\n",
    "tokenized_texts = [text.split() for text in biden_df['processed_tweet']]\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "\n",
    "# Create a bag-of-words corpus from dictionary representation.\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "# Train an LDA model with K=30 topics.\n",
    "lda_model_normal = models.LdaModel(corpus, num_topics=30, id2word=dictionary, passes=200)\n",
    "\n",
    "# Display topics found by the model:\n",
    "topics_normal = lda_model_normal.print_topics(num_words=5)\n",
    "for topic in topics_normal:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "# Train a RollingLDA on the corpus. Set the time chunk length to three days and choose K = 30. If this takes a lot of time, chose prototype=1 and lower the epoch count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ttta\n",
      "  Downloading ttta-0.9.5.tar.gz (90 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (3.8.1)\n",
      "Collecting Cython (from ttta)\n",
      "  Using cached Cython-3.0.11-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pandas in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (2.1.3)\n",
      "Requirement already satisfied: gensim in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (4.3.3)\n",
      "Requirement already satisfied: matplotlib in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (3.9.4)\n",
      "Collecting spacy (from ttta)\n",
      "  Downloading spacy-3.8.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: joblib in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (1.3.2)\n",
      "Requirement already satisfied: scipy in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (1.11.4)\n",
      "Requirement already satisfied: numpy in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (1.26.2)\n",
      "Requirement already satisfied: tqdm in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (4.66.1)\n",
      "Requirement already satisfied: seaborn in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (0.13.2)\n",
      "Requirement already satisfied: pyyaml in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (6.0.2)\n",
      "Collecting xmltodict (from ttta)\n",
      "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: scikit-learn in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from ttta) (1.3.2)\n",
      "Collecting transformers>=4.46.3 (from ttta)\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch (from ttta)\n",
      "  Downloading torch-2.6.0-cp39-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting HanTa (from ttta)\n",
      "  Downloading HanTa-1.1.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting wasabi (from ttta)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting pyLDAvis>=3.4.0 (from ttta)\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from pyLDAvis>=3.4.0->ttta) (3.1.4)\n",
      "Collecting numexpr (from pyLDAvis>=3.4.0->ttta)\n",
      "  Downloading numexpr-2.10.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting funcy (from pyLDAvis>=3.4.0->ttta)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from pyLDAvis>=3.4.0->ttta) (58.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from pandas->ttta) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from pandas->ttta) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from pandas->ttta) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from scikit-learn->ttta) (3.2.0)\n",
      "Collecting filelock (from transformers>=4.46.3->ttta)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers>=4.46.3->ttta)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from transformers>=4.46.3->ttta) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from transformers>=4.46.3->ttta) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from transformers>=4.46.3->ttta) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.46.3->ttta)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.46.3->ttta)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from gensim->ttta) (7.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from matplotlib->ttta) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from matplotlib->ttta) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from matplotlib->ttta) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from matplotlib->ttta) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from matplotlib->ttta) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from matplotlib->ttta) (3.2.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from matplotlib->ttta) (6.5.2)\n",
      "Requirement already satisfied: click in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from nltk->ttta) (8.1.7)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->ttta)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->ttta)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->ttta)\n",
      "  Downloading murmurhash-1.0.12-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->ttta)\n",
      "  Downloading cymem-2.0.11-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->ttta)\n",
      "  Downloading preshed-3.0.9-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy->ttta)\n",
      "  Downloading thinc-8.3.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->ttta)\n",
      "  Downloading srsly-2.5.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->ttta)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy->ttta)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy->ttta)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy->ttta)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->ttta)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch->ttta)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch->ttta)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting fsspec (from torch->ttta)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch->ttta)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->ttta)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib->ttta) (3.17.0)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy->ttta)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->ttta)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->ttta)\n",
      "  Downloading pydantic_core-2.27.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->ttta) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from requests->transformers>=4.46.3->ttta) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from requests->transformers>=4.46.3->ttta) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from requests->transformers>=4.46.3->ttta) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from requests->transformers>=4.46.3->ttta) (2023.11.17)\n",
      "Requirement already satisfied: wrapt in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from smart-open>=1.8.1->gensim->ttta) (1.17.2)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.0->spacy->ttta)\n",
      "  Downloading blis-1.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy->ttta)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy->ttta)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy->ttta)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy->ttta)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from jinja2->pyLDAvis>=3.4.0->ttta) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->ttta)\n",
      "  Downloading marisa_trie-1.2.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->ttta)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/oayanwale/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->ttta) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->ttta)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached Cython-3.0.11-py2.py3-none-any.whl (1.2 MB)\n",
      "Downloading HanTa-1.1.1-py3-none-any.whl (15.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.8.3-cp39-cp39-macosx_11_0_arm64.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading torch-2.6.0-cp39-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp39-cp39-macosx_11_0_arm64.whl (42 kB)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp39-cp39-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp39-cp39-macosx_11_0_arm64.whl (129 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp39-cp39-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp39-cp39-macosx_11_0_arm64.whl (635 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp39-cp39-macosx_11_0_arm64.whl (780 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.8/780.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Downloading numexpr-2.10.2-cp39-cp39-macosx_11_0_arm64.whl (134 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.2.0-cp39-cp39-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp39-cp39-macosx_11_0_arm64.whl (175 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: ttta\n",
      "  Building wheel for ttta (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ttta: filename=ttta-0.9.5-cp39-cp39-macosx_10_9_universal2.whl size=164946 sha256=06531031f34639d3055753f998948f2d81e92cfd81461210f070bc5b02bfb10d\n",
      "  Stored in directory: /Users/oayanwale/Library/Caches/pip/wheels/ce/ff/90/b3825333ec6bfaefa310345dbe32f0017e4ce304c388acbf36\n",
      "Successfully built ttta\n",
      "Installing collected packages: mpmath, HanTa, funcy, cymem, xmltodict, wasabi, typing-extensions, sympy, spacy-loggers, spacy-legacy, shellingham, safetensors, numexpr, networkx, murmurhash, mdurl, marisa-trie, fsspec, filelock, Cython, catalogue, blis, annotated-types, torch, srsly, pydantic-core, preshed, markdown-it-py, language-data, huggingface-hub, cloudpathlib, tokenizers, rich, pyLDAvis, pydantic, langcodes, typer, transformers, confection, weasel, thinc, spacy, ttta\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed Cython-3.0.11 HanTa-1.1.1 annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 filelock-3.17.0 fsspec-2024.12.0 funcy-2.0 huggingface-hub-0.28.1 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 murmurhash-1.0.12 networkx-3.2.1 numexpr-2.10.2 preshed-3.0.9 pyLDAvis-3.4.1 pydantic-2.10.6 pydantic-core-2.27.2 rich-13.9.4 safetensors-0.5.2 shellingham-1.5.4 spacy-3.8.3 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 sympy-1.13.1 thinc-8.3.4 tokenizers-0.21.0 torch-2.6.0 transformers-4.48.2 ttta-0.9.5 typer-0.15.1 typing-extensions-4.12.2 wasabi-1.1.3 weasel-0.4.1 xmltodict-0.14.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ttta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ttta.methods.rolling_lda import RollingLDA\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary downloads\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RollingLDA in module ttta.methods.rolling_lda:\n",
      "\n",
      "class RollingLDA(builtins.object)\n",
      " |  RollingLDA(K: int, how: Union[str, List[datetime.datetime]] = 'ME', warmup: int = 48, memory: int = 3, alpha: float = None, gamma: float = None, initial_epochs: int = 100, subsequent_epochs: int = 50, min_count: int = 2, max_assign=False, prototype: int = 10, topic_threshold: List[Union[int, float]] = None, prototype_measure: Union[str, Callable] = 'jaccard', lda: ttta.methods.lda_prototype.LDAPrototype = None, min_docs_per_chunk: int = None, verbose: int = 1, seed: Union[int, numpy.uint32] = None) -> None\n",
      " |  \n",
      " |  Implements a rolling LDA model for diachronic topic modeling.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, K: int, how: Union[str, List[datetime.datetime]] = 'ME', warmup: int = 48, memory: int = 3, alpha: float = None, gamma: float = None, initial_epochs: int = 100, subsequent_epochs: int = 50, min_count: int = 2, max_assign=False, prototype: int = 10, topic_threshold: List[Union[int, float]] = None, prototype_measure: Union[str, Callable] = 'jaccard', lda: ttta.methods.lda_prototype.LDAPrototype = None, min_docs_per_chunk: int = None, verbose: int = 1, seed: Union[int, numpy.uint32] = None) -> None\n",
      " |      Initialize a RollingLDA model.\n",
      " |      \n",
      " |      Args:\n",
      " |          K: The number of topics.\n",
      " |          how: List of datetime dates indicating the end of time chunks or a string indicating the frequency of the time chunks as in pandas.resample().\n",
      " |          warmup: The number of chunks to use for the initial fitting period of RollinglDA.\n",
      " |          memory: The number of chunks to look back for when calculating the topic proportions for the RollingLDA.\n",
      " |          alpha: The alpha parameter of the LDA model.\n",
      " |          gamma: The gamma parameter of the LDA model.\n",
      " |          initial_epochs: The number of epochs to train the LDA model in the initial fit.\n",
      " |          subsequent_epochs: The number of epochs to train the LDA model in the subsequent fits.\n",
      " |          min_count: The minimum number of times a word must occur in the corpus to be included in the vocabulary.\n",
      " |          max_assign: Whether the maximum number of topic assignments per document should be used as the final topic assignment.\n",
      " |          prototype: Number of LDAs to fit and to choose a Prototype from.\n",
      " |          topic_threshold: The minimum occurrences within a topic to be relevant for the prototype similarity calculation.\n",
      " |          prototype_measure: The measure to use for calculating the distance between the LDAs. Can be \"jaccard\", \"cosine\" or a custom function.\n",
      " |          lda: An LDAPrototype model to use instead of fitting a new one.\n",
      " |          min_docs_per_chunk: The minimum number of documents a chunk must contain to be used for the LDA training.\n",
      " |          verbose: The verbosity of the output. 0 does not show any output, 1 shows a progress bar, 2 also shows information relevant for debugging.\n",
      " |          seed: A seed for random number generation\n",
      " |  \n",
      " |  fit(self, texts: pandas.core.frame.DataFrame, workers: int = 1, text_column: str = 'text', date_column: str = 'date') -> None\n",
      " |      Fits a RollingLDA model in place to the given texts from scratch.\n",
      " |      \n",
      " |      When updating an existing model, use fit_update() instead.\n",
      " |      Args:\n",
      " |          texts: A pandas DataFrame containing the columns text_column and date_column containing the documents and their respective dates.\n",
      " |                 The dates must be in a format interpretable by pandas.to_datetime(). Each element of the text_column column must be a list of strings.\n",
      " |          workers: The number of workers to use for parallelization.\n",
      " |          text_column: The name of the column in texts containing the documents.\n",
      " |          date_column: The name of the column in texts containing the dates.\n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  fit_update(self, texts: pandas.core.frame.DataFrame, how: Union[str, List[datetime.datetime]] = None, workers: int = 1, text_column: str = 'text', date_column: str = 'date') -> None\n",
      " |      Update the fit of a RollingLDA model in place to the given texts from scratch.\n",
      " |      When training a model from scratch, fit() is called instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts: A pandas DataFrame containing the columns text_column and date_column containing the documents and their respective dates.\n",
      " |                 The dates must be in a format interpretable by pandas.to_datetime(). Each element of the text_column column must be a list of strings.\n",
      " |          how: List of datetime dates indicating the end of time chunks or a string indicating the frequency of the time chunks as in pandas.resample().\n",
      " |          workers: The number of workers to use for parallelization.\n",
      " |          text_column: The name of the column in texts containing the documents.\n",
      " |          date_column: The name of the column in texts containing the dates.\n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  get_date_of_chunk(self, chunk: int = None) -> Union[pandas.core.frame.DataFrame, str]\n",
      " |      Return the time span that a chunk represents. Returns a pandas\n",
      " |      DataFrame of all chunks with the chunk as the index if chunk is None or\n",
      " |      \"all\".\n",
      " |      \n",
      " |      Args:\n",
      " |          chunk: The chunk for which the time span should be returned.\n",
      " |      Returns:\n",
      " |          The time span that the chunk represents or a DataFrame of all\n",
      " |          chunks and their time spans.\n",
      " |  \n",
      " |  get_document_topic_matrix(self, chunk: int = None) -> numpy.ndarray\n",
      " |      Return the document-topic matrix for the given chunk.\n",
      " |      \n",
      " |      If no chunk is given, the document-topic matrix for all chunks is returned.\n",
      " |      Args:\n",
      " |          chunk: The chunk for which the document-topic matrix should be returned. If None, the document-topic matrix for all chunks is returned.\n",
      " |      Returns:\n",
      " |          The document-topic matrix for the given chunk or all chunks.\n",
      " |  \n",
      " |  get_highest_topic_share(self, topic: int, chunk: int = None, number: int = 5, min_length: int = 10) -> pandas.core.frame.DataFrame\n",
      " |      Return the documents with the highest topic shares for the given chunk or overall.\n",
      " |      \n",
      " |      Args:\n",
      " |          topic: The topic for which the documents with the highest topic shares should be returned.\n",
      " |          chunk: The chunk for which the documents with the highest topic shares should be returned.\n",
      " |                   If None, the documents with the highest topic shares over the entire time frame are returned.\n",
      " |          number: The number of documents to return.\n",
      " |          min_length: The minimum number of words a document must contain to be considered.\n",
      " |      Returns:\n",
      " |          The documents with the highest topic shares for the given chunk.\n",
      " |  \n",
      " |  get_parameters(self) -> dict\n",
      " |      Return the parameters of the RollingLDA model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary containing the parameters of the RollingLDA model.\n",
      " |  \n",
      " |  get_word_topic_matrix(self, chunk: int = None) -> numpy.ndarray\n",
      " |      Return the topic assignments for the given chunk.\n",
      " |      \n",
      " |      If no chunk is given, the topic assignments for all chunks are returned.\n",
      " |      Args:\n",
      " |          chunk: The chunk for which the topic assignments should be returned. If None, the topic assignments for all chunks are returned.\n",
      " |      Returns:\n",
      " |          The topic assignments for the given chunk or all chunks.\n",
      " |  \n",
      " |  load(self, path: str) -> None\n",
      " |      Load a pickled RollingLDA model from the given path.\n",
      " |      \n",
      " |      Args:\n",
      " |          path: The path from which the RollingLDA model should be loaded.\n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  save(self, path: str) -> None\n",
      " |      Save the RollingLDA model to the given path as a .pickle-file.\n",
      " |      \n",
      " |      Args:\n",
      " |          path: The path to which the RollingLDA model should be saved.\n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  set_parameters(self, parameters: dict) -> None\n",
      " |      Set the parameters of the RollingLDA model.\n",
      " |      \n",
      " |      Args:\n",
      " |          parameters: A dictionary containing the parameters of the RollingLDA model.\n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  top_words(self, chunk: Union[int, str] = None, topic: int = None, number: int = 5, importance: bool = True, return_as_data_frame: bool = True) -> Union[List[str], List[List[str]]]\n",
      " |      Return the top words for the given chunk and topic.\n",
      " |      \n",
      " |      Args:\n",
      " |          chunk: The chunk for which the top words should be returned. If None, the top words over the entire time frame are returned.\n",
      " |                 If \"all\", the top words for each individual chunk are returned.\n",
      " |          topic: The topic for which the top words should be returned. If None, the top words for all topics are returned.\n",
      " |          number: The number of top words to return.\n",
      " |          importance: Whether the words should be weighted based on their importance to a topic or their absolute frequency.\n",
      " |          return_as_data_frame: Whether the top words should be returned as a pandas DataFrame.\n",
      " |  \n",
      " |  topic_evolution(self, topic: int = None, path: str = None, show: bool = True, figsize: Tuple[int, int] = (15, 5)) -> None\n",
      " |      Plot the evolution of the topic shares over time.\n",
      " |      \n",
      " |      Args:\n",
      " |          topic: The topic for which the evolution should be plotted. If None, the evolution for all topics is plotted.\n",
      " |          path: The path to save the plot to.\n",
      " |          show: Whether to show the plot.\n",
      " |          figsize: The size of the plot.\n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  topic_shares(self, index: int = None, by: str = 'chunk') -> pandas.core.frame.DataFrame\n",
      " |      Return the topic shares for the given chunk or document.\n",
      " |      \n",
      " |      Args:\n",
      " |          index: The index for which the topic shares should be returned.\n",
      " |          by: Whether the topic shares should be returned by \"chunk\" or \"document\".\n",
      " |      Returns:\n",
      " |          The topic shares for the given chunk or all chunks.\n",
      " |  \n",
      " |  visualize(self, chunk: int = None, number: int = 30, path: str = 'ldaviz.html', open_browser: bool = True) -> None\n",
      " |      Visualize the RollingLDA model using pyLDAvis.\n",
      " |      Args:\n",
      " |          chunk: The chunk for which the visualization should be created.\n",
      " |                 If None, the visualization is based on all chunks.\n",
      " |          number: The number of top words to include in the visualization.\n",
      " |          path: The path to save the visualization to.\n",
      " |          open_browser: Whether to open the visualization in the browser.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  wordclouds(self, chunks: List[int] = None, topic: int = None, number: int = 50, path: str = 'wordclouds', height: int = 500, width: int = 700, show: bool = True)\n",
      " |      Plot the wordclouds for the given chunk.\n",
      " |      \n",
      " |      If chunk is None, the wordclouds are plotted for every time chunk.\n",
      " |      Args:\n",
      " |          chunks: The chunks for which the wordclouds should be plotted. If None, the wordclouds over the entire time frame are plotted.\n",
      " |          topic: The topic to create a word cloud for or None for all topics\n",
      " |          number: number of top words to include in the word cloud\n",
      " |          path: path to a directory to store the wordcloud files in\n",
      " |          width: width of the image\n",
      " |          height: height of the image\n",
      " |          show: should the image be shown\n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RollingLDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique dates in dataset: 117243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 8937 documents in chunk 2020-10-30: : 0chunk [00:00, ?chunk/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m rolling_lda_model \u001b[38;5;241m=\u001b[39m rolling_lda\u001b[38;5;241m.\u001b[39mRollingLDA(K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m     13\u001b[0m                                             how\u001b[38;5;241m=\u001b[39mhow,  \n\u001b[1;32m     14\u001b[0m                                             warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m5\u001b[39m, unique_dates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# Ensure warmup is less than available chunks\u001b[39;00m\n\u001b[1;32m     15\u001b[0m                                             memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Fit the model using the entire DataFrame and specify column names\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mrolling_lda_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbiden_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_tweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdate_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreated_at\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ttta/methods/rolling_lda.py:207\u001b[0m, in \u001b[0;36mRollingLDA.fit\u001b[0;34m(self, texts, workers, text_column, date_column)\u001b[0m\n\u001b[1;32m    204\u001b[0m     iterator\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_indices\u001b[38;5;241m.\u001b[39miloc[i\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_start\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_indices\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_start\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m                              \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_indices\u001b[38;5;241m.\u001b[39miloc[i][date_column]\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup:  \u001b[38;5;66;03m# fit warmup chunks\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mchunk_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_warmup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    210\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_indices) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_indices\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_start\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ttta/methods/lda_prototype.py:258\u001b[0m, in \u001b[0;36mLDAPrototype.fit\u001b[0;34m(self, texts, epochs, first_chunk, chunk_end, memory_start, workers)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(texts[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    257\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    259\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m text] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m    260\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe given texts are not input as strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from ttta.methods import rolling_lda\n",
    "\n",
    "# Set parameters for Rolling LDA\n",
    "time_chunk_length_days = 3  # Length of time chunk in days\n",
    "how = f'{time_chunk_length_days}D'  # Use '3D' for three-day chunks\n",
    "\n",
    "# Check how many unique dates are in your dataset\n",
    "unique_dates = biden_df['created_at'].nunique()\n",
    "print(\"Unique dates in dataset:\", unique_dates)\n",
    "\n",
    "# Adjust warmup if necessary based on unique dates\n",
    "rolling_lda_model = rolling_lda.RollingLDA(K=30,\n",
    "                                            how=how,  \n",
    "                                            warmup=min(5, unique_dates - 1),  # Ensure warmup is less than available chunks\n",
    "                                            memory=3)\n",
    "\n",
    "# Fit the model using the entire DataFrame and specify column names\n",
    "rolling_lda_model.fit(biden_df, \n",
    "                       workers=1, \n",
    "                       text_column='processed_tweet', \n",
    "                       date_column='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 129996\n"
     ]
    }
   ],
   "source": [
    "# Check if biden_df has any rows\n",
    "print(\"Number of tweets:\", len(biden_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tweets Sample:\n",
      " 86581                                                   []\n",
      "10717    [twitter, manipulate, u, election, favor, ccp,...\n",
      "11909    [fbi, allegedly, obtained, hunter, biden, comp...\n",
      "69045    [isnt, sellout, long, black, people, going, vo...\n",
      "34453               [im, going, share, thing, like, biden]\n",
      "Name: processed_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check processed_tweet column for any entries\n",
    "print(\"Processed Tweets Sample:\\n\", biden_df['processed_tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweets with Empty Processed Results:\n",
      "                                                     tweet processed_tweet\n",
      "86581   #censorship #HunterBiden #Biden #BidenEmails #...              []\n",
      "88313            #IOWA FOR #BIDEN https://t.co/sZbgS5DSVg              []\n",
      "72578   @JoeBiden #Truth #COVID19 #Trump #Biden #Harri...              []\n",
      "93156   #trump #biden #nypost #HunterBiden #ElectionTw...              []\n",
      "57510   #Biden/Harris2020 \\n#VoteLikeYourLifeDependsOn...              []\n",
      "...                                                   ...             ...\n",
      "77096     #MAGA #BIDEN #TRUMPLOST https://t.co/eGHaSPnqHL              []\n",
      "10383   #USElectionResults2020 #USElection #JoeBiden #...              []\n",
      "61998   https://t.co/9pTKZHPxed\\n\\n#Election2020result...              []\n",
      "129808  🙏🙌🤩🤩 #Biden #Harris \\n#USAElections2020 🇺🇸 htt...              []\n",
      "63212   @realDonaldTrump #Celebration #Biden https://t...              []\n",
      "\n",
      "[7722 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find indices of empty processed tweets\n",
    "empty_processed_indices = biden_df[biden_df['processed_tweet'].map(len) == 0].index\n",
    "\n",
    "# Display the original tweets corresponding to these indices\n",
    "empty_tweets = biden_df.loc[empty_processed_indices]\n",
    "print(\"Original Tweets with Empty Processed Results:\\n\", empty_tweets[['tweet', 'processed_tweet']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: 7,722 tweets with empty processed results\n",
    "The original tweets contain hashtags, mentions, and links, which are likely being removed during preprocessing. This is why the processed text results in empty lists.\n",
    "\n",
    "# Solution\n",
    "adjust preprocessing function to ensure that meaningful content is retained even when it contains hashtags or mentions.\n",
    "\n",
    "1. Retain Hashtags and Mentions: Instead of removing all non-alphabetic characters, you could modify your regex to keep hashtags and mentions as they may carry sentiment or meaning.\n",
    "\n",
    "2. Custom Stop Words: If certain words are being filtered out but are important (like \"u\"), consider adding them back into your processing logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjustment to Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   tweet  \\\n",
      "86581  #censorship #HunterBiden #Biden #BidenEmails #...   \n",
      "10717  In 2020, #NYPost is being #censorship #CENSORE...   \n",
      "11909  FBI Allegedly Obtained Hunter Biden Computer, ...   \n",
      "69045  #IceCube isn’t a sellout how long are black pe...   \n",
      "34453  I’m going to share things I like about Biden m...   \n",
      "\n",
      "                                         processed_tweet  \n",
      "86581  #censorship #hunterbiden #biden #bidenemails #...  \n",
      "10717  #nypost #censorship #censored twitter manipula...  \n",
      "11909  fbi allegedly obtained hunter biden computer d...  \n",
      "69045  #icecube sellout long black people going vote ...  \n",
      "34453  going share thing like biden #bidencares #bide...  \n"
     ]
    }
   ],
   "source": [
    "# # Adjustment to Task 2: Revised Preprocessing Function\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs but retain hashtags and mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    \n",
    "    # Keep hashtags and mentions; only remove punctuation (except for '#' and '@')\n",
    "    text = re.sub(r'[^a-zA-Z\\s#@]', ' ', text)\n",
    "    \n",
    "    # Tokenize (split into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords while keeping meaningful terms like 'u'\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Reapply preprocessing after adjustments\n",
    "biden_df['processed_tweet'] = biden_df['tweet'].apply(preprocess_text)\n",
    "\n",
    "# Verify again after reprocessing\n",
    "print(biden_df[['tweet', 'processed_tweet']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tweets Sample:\n",
      " 86581    #censorship #hunterbiden #biden #bidenemails #...\n",
      "10717    #nypost #censorship #censored twitter manipula...\n",
      "11909    fbi allegedly obtained hunter biden computer d...\n",
      "69045    #icecube sellout long black people going vote ...\n",
      "34453    going share thing like biden #bidencares #bide...\n",
      "Name: processed_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check processed_tweet column for any entries\n",
    "print(\"Processed Tweets Sample:\\n\", biden_df['processed_tweet'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rerun Task 3: Train a Normal LDA Model above \n",
    "# now we will rerun Task 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available time chunks: 8\n"
     ]
    }
   ],
   "source": [
    "# Count total number of chunks\n",
    "biden_df[\"days_since_start\"] = (biden_df[\"created_at\"] - biden_df[\"created_at\"].min()).dt.days\n",
    "total_chunks = biden_df[\"days_since_start\"].max() // 3  # Divide by 3 to get 3-day chunks\n",
    "\n",
    "print(f\"Total available time chunks: {total_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_tweet\n",
      "<class 'list'>    129996\n",
      "Name: count, dtype: int64\n",
      "86581                                                   []\n",
      "10717    [twitter, manipulate, u, election, favor, ccp,...\n",
      "11909    [fbi, allegedly, obtained, hunter, biden, comp...\n",
      "69045    [isnt, sellout, long, black, people, going, vo...\n",
      "34453               [im, going, share, thing, like, biden]\n",
      "Name: processed_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(biden_df[\"processed_tweet\"].apply(type).value_counts())  # Should all be <class 'list'>\n",
    "print(biden_df[\"processed_tweet\"].head())  # Check if they contain lists of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error message TypeError: The elements of the 'texts' column of texts must each contain a tokenized document as a list of strings! indicates that the Rolling LDA model expects the processed_tweet column in your DataFrame to contain lists of tokens (i.e., words), but it seems it is currently formatted as a single string.\n",
    "\n",
    "Steps to Fix\n",
    "Ensure Tokenization: When you preprocess your tweets, make sure that the processed text is stored as a list of words instead of a single string.\n",
    "Update Preprocessing Function: Modify the preprocessing function so that it retains the tokenized format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0  #ElectionNight #MSNBC2020 #IVoted #Biden2020 #...   \n",
      "1  Go Headlines: #TopNews Of The Hour\\n#USElectio...   \n",
      "2  I doubt the person(s) who stole our official B...   \n",
      "3  The Bidens are safe so long as Fox News is the...   \n",
      "4  Since I live in a republican state TIME FOR ME...   \n",
      "\n",
      "                                     processed_tweet  \n",
      "0  [electionnight, msnbc, ivoted, biden, bluewave...  \n",
      "1  [go, headline, topnews, hour, uselection, trum...  \n",
      "2  [doubt, person, stole, official, biden, harris...  \n",
      "3  [bidens, safe, long, fox, news, news, outlet, ...  \n",
      "4  [since, live, republican, state, time, start, ...  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset (biden.csv)\n",
    "file_path = '/Users/oayanwale/Downloads/NLP_Exercise_24_25/Data/biden.csv'\n",
    "biden_df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'created at' is in datetime format\n",
    "biden_df['created_at'] = pd.to_datetime(biden_df['created_at'])\n",
    "\n",
    "# Preprocess tweets as previously defined.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs and punctuation/numbers, keeping only alphabetic characters and spaces\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Keep only letters and spaces\n",
    "    \n",
    "    # Tokenize (split into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize remaining words\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return processed_words  # Return as a list of tokens\n",
    "\n",
    "# Apply preprocessing to tweet texts column (assuming it's named 'tweet')\n",
    "biden_df['processed_tweet'] = biden_df['tweet'].apply(preprocess_text)\n",
    "\n",
    "# Check if processed_tweet contains lists of tokens correctly formatted:\n",
    "print(biden_df[['tweet', 'processed_tweet']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 24653 documents in chunk 2020-11-02: : 1chunk [00:56, 56.39s/chunk]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'coo_matrix' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m rolling_lda_model \u001b[38;5;241m=\u001b[39m rolling_lda\u001b[38;5;241m.\u001b[39mRollingLDA(K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                             how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_chunk_length_days\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m,  \n\u001b[1;32m      9\u001b[0m                                             warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,   \n\u001b[1;32m     10\u001b[0m                                             memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Fit the model using the entire DataFrame and specify column names \u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mrolling_lda_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbiden_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_tweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdate_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreated_at\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Display top terms from fitted model:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopics from Rolling LDA model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ttta/methods/rolling_lda.py:211\u001b[0m, in \u001b[0;36mRollingLDA.fit\u001b[0;34m(self, texts, workers, text_column, date_column)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_indices) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_indices\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_start\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_subsequent_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_text \u001b[38;5;241m=\u001b[39m {date_column: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ttta/methods/lda_prototype.py:303\u001b[0m, in \u001b[0;36mLDAPrototype.fit\u001b[0;34m(self, texts, epochs, first_chunk, chunk_end, memory_start, workers)\u001b[0m\n\u001b[1;32m    300\u001b[0m old_dtm_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dtm(texts)\n\u001b[0;32m--> 303\u001b[0m word_vec, doc_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_word_and_doc_vector(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mold_dtm_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mchunk_end\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mold_dtm_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord_vec length: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(word_vec))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'coo_matrix' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from ttta.methods import rolling_lda  # Ensure this imports correctly\n",
    "\n",
    "# Set parameters for Rolling LDA\n",
    "time_chunk_length_days = 3  # Length of time chunk in days\n",
    "\n",
    "# Initialize Rolling LDA model with appropriate parameters.\n",
    "rolling_lda_model = rolling_lda.RollingLDA(K=30,\n",
    "                                            how=f'{time_chunk_length_days}D',  \n",
    "                                            warmup=5,   \n",
    "                                            memory=3)\n",
    "\n",
    "# Fit the model using the entire DataFrame and specify column names \n",
    "rolling_lda_model.fit(biden_df, \n",
    "                       workers=1, \n",
    "                       text_column='processed_tweet', \n",
    "                       date_column='created_at')\n",
    "\n",
    "# Display top terms from fitted model:\n",
    "print(\"Topics from Rolling LDA model:\")\n",
    "for i in range(30):  # Assuming K=30 topics\n",
    "    print(f\"Topic {i + 1}: {rolling_lda_model.get_top_words(topic=i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RollingLDA' object has no attribute '_dtm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mfit_rolling_lda_with_dense_dtm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrolling_lda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiden_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_tweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreated_at\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Display top terms from fitted model:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopics from Rolling LDA model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[59], line 17\u001b[0m, in \u001b[0;36mfit_rolling_lda_with_dense_dtm\u001b[0;34m(model, data, workers, text_column, date_column)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_rolling_lda_with_dense_dtm\u001b[39m(model, data, workers, text_column, date_column):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Ensure DTM is in dense format before fitting\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     model\u001b[38;5;241m.\u001b[39m_dtm \u001b[38;5;241m=\u001b[39m convert_dtm_to_dense(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtm\u001b[49m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Fit the model on the data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(data, workers\u001b[38;5;241m=\u001b[39mworkers, text_column\u001b[38;5;241m=\u001b[39mtext_column, date_column\u001b[38;5;241m=\u001b[39mdate_column)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RollingLDA' object has no attribute '_dtm'"
     ]
    }
   ],
   "source": [
    "# Set parameters for Rolling LDA\n",
    "time_chunk_length_days = 3  # Length of time chunk in days\n",
    "\n",
    "# Initialize Rolling LDA model with appropriate parameters\n",
    "rolling_lda_model = rolling_lda.RollingLDA(K=30,\n",
    "                                           how=f'{time_chunk_length_days}D',  \n",
    "                                           warmup=5,   \n",
    "                                           memory=3)\n",
    "\n",
    "# Helper function to convert the DTM to a dense format before fitting\n",
    "def convert_dtm_to_dense(dtm):\n",
    "    return dtm.todense()  # Convert to dense matrix for easier access\n",
    "\n",
    "# Helper function to fit the model with conversion to dense DTM\n",
    "def fit_rolling_lda_with_dense_dtm(model, data, workers, text_column, date_column):\n",
    "    # Ensure DTM is in dense format before fitting\n",
    "    model._dtm = convert_dtm_to_dense(model._dtm)\n",
    "    \n",
    "    # Fit the model on the data\n",
    "    model.fit(data, workers=workers, text_column=text_column, date_column=date_column)\n",
    "    \n",
    "    # No need to convert DTM after fitting since it's already in dense format\n",
    "    return model\n",
    "\n",
    "# Fit the model\n",
    "fit_rolling_lda_with_dense_dtm(rolling_lda_model, biden_df, workers=1, text_column='processed_tweet', date_column='created_at')\n",
    "\n",
    "# Display top terms from fitted model:\n",
    "print(\"Topics from Rolling LDA model:\")\n",
    "for i in range(30):  # Assuming K=30 topics\n",
    "    print(f\"Topic {i + 1}: {rolling_lda_model.get_top_words(topic=i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "tweets_file = \"/Users/user/Downloads/NLP/NLP_Exercise/Data/biden.csv\"\n",
    "tweets_data = pd.read_csv(tweets_file)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', text)  # Remove URLs, hashtags, mentions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "# Apply preprocessing\n",
    "tweets_data['clean_text'] = tweets_data['tweet'].apply(preprocess_text)\n",
    "# Convert text into Bag of Words (BoW)\n",
    "dictionary = Dictionary(tweets_data['clean_text'])\n",
    "corpus = [dictionary.doc2bow(text) for text in tweets_data['clean_text']]\n",
    "\n",
    "# Train LDA model\n",
    "num_topics = 30\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "\n",
    "# Print top words for each topic\n",
    "def display_topics(model, num_words):\n",
    "    for idx, topic in model.show_topics(formatted=False, num_words=num_words):\n",
    "        print(f\"Topic {idx+1}: {', '.join([word for word, _ in topic])}\")\n",
    "print(\"LDA Topics:\")\n",
    "display_topics(lda_model, 10)\n",
    "\n",
    "# Plot topic distribution\n",
    "topic_distribution = [max(lda_model[doc], key=lambda x: x[1])[0] for doc in corpus]\n",
    "plt.hist(topic_distribution, bins=num_topics, alpha=0.75)\n",
    "plt.xlabel(\"Topics\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.title(\"LDA Topic Distribution\")\n",
    "plt.show()\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
