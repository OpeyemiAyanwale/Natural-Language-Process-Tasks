{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise sheet 13 with the movie.txt Load the data set into your console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In moodle you will find the file movies.txt. In it, you will find summaries of different movies. We do however not know the genres of these books. Your task is to perform an unsupervised analysis to make an educated guess, which genres might be part of the data set.\n",
    "\n",
    "This is an open exercise. You will not be tasked with a specific model to use. Instead, you can use any model we have talked about in the exercises and the lecture so far to solve this task, except for Transformer models. This exercise sheet can be seen as a preperation for the practical part of the exam: You should be able to solve this exercise within 60 minutes on your own, without the help of any AI-assistant (you are allowed to use your previous solutions and Google though).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1\n",
    "Argue the usefulness of the following models for the task at hand in 2-3 sen- tences:\n",
    "• Dictionary-based analysis\n",
    "• Latent Dirichlet Allocation\n",
    "• Word2Vec\n",
    "Which method do you deem to be the best for this task?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Dictionary-based analysis: Simple and interpretable and Works well if we have predefined lists of genre-related words.\n",
    "Cons:\n",
    "Requires a genre-specific dictionary, which we don’t have.\n",
    "Struggles with polysemy (words with multiple meanings).\n",
    "Cannot discover new themes; only matches predefined words.\n",
    "\n",
    "Verdict: Not ideal, as we aim for unsupervised learning without a predefined dictionary.\n",
    "\n",
    "• Latent Dirichlet Allocation: Topic modeling approach that discovers hidden themes in text. Suitable for unsupervised genre detection. Can identify multiple genres per movie.\n",
    "Cons: Requires choosing the number of topics beforehand.\n",
    "Struggles if summaries are too short or contain noise.\n",
    "\n",
    "Verdict: A strong choice for discovering genres in an unsupervised way.\n",
    "\n",
    "• Word2Vec: Captures word meanings based on context.\n",
    "Can cluster similar movie summaries together based on semantic meaning.\n",
    "Cons:\n",
    "Requires a large dataset to train meaningful embeddings.\n",
    "Does not directly provide topic categories.\n",
    "\n",
    "Verdict: Good for clustering but not ideal for direct topic modeling.\n",
    "\n",
    "Best Method: LDA\n",
    "Latent Dirichlet Allocation (LDA) is the best choice because it allows us to extract hidden topics (potential genres) from movie summaries without any labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "# Preprocess the data so that it is fit for the pipeline you intend to solve the task with. Shortly explain every step of preprocessing and why it is useful for this analysis.\n",
    "\n",
    "\n",
    "Preprocessing Steps:\n",
    "✔ Lowercasing: To ensure consistency (\"Action\" and \"action\" are treated the same).\n",
    "✔ Removing Punctuation & Special Characters: LDA works with word distributions, not punctuation.\n",
    "✔ Tokenization: Split text into words.\n",
    "✔ Removing Stopwords: Words like \"the\", \"is\", \"and\" add no meaning.\n",
    "✔ Lemmatization: Convert words to their base form (\"running\" → \"run\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text file\n",
    "with open('/Users/oayanwale/Downloads/NLP_Exercise_24_25/movies.txt', 'r', encoding='utf-8') as file:\n",
    "    movie_summaries = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oayanwale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenization\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_summaries = [preprocess_text(summary) for summary in movie_summaries]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Solve the task by creating a pipeline with the model you deem to be optimal for this task.\n",
    "\n",
    "\n",
    "Applying LDA for Genre Detection\n",
    "Now, we’ll use LDA to extract topics from movie summaries.\n",
    "\n",
    "Steps for LDA Pipeline:\n",
    "✔ Convert processed text into a bag-of-words representation.\n",
    "✔ Train an LDA model with an optimal number of topics.\n",
    "✔ Analyze the topics to infer movie genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "['take', 'year', 'one', 'young', 'must', 'world', 'new', 'family', 'find', 'life']\n",
      "\n",
      "Topic 2:\n",
      "['go', 'love', 'story', 'young', 'find', 'get', 'take', 'world', 'two', 'one']\n",
      "\n",
      "Topic 3:\n",
      "['murder', 'mysterious', 'get', 'family', 'young', 'two', 'friend', 'life', 'new', 'find']\n",
      "\n",
      "Topic 4:\n",
      "['new', 'town', 'find', 'friend', 'love', 'must', 'two', 'one', 'young', 'life']\n",
      "\n",
      "Topic 5:\n",
      "['school', 'one', 'man', 'girl', 'get', 'young', 'find', 'woman', 'new', 'life']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Convert text into a bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "text_matrix = vectorizer.fit_transform(processed_summaries)\n",
    "\n",
    "# Train LDA model\n",
    "num_topics = 5  # Adjust based on dataset\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(text_matrix)\n",
    "\n",
    "# Display topics\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print([words[i] for i in topic.argsort()[-10:]])  # Show top 10 words\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 1: Likely Genre: Drama / Coming-of-Age\n",
    "Topic 2: Likely Genre: Romance / Adventure\n",
    "Topic 3: Likely Genre: Mystery / Thriller\n",
    "Topic 4: Likely Genre: Comedy / Slice of Life\n",
    "Topic 5: Likely Genre: Teen Drama / Romance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA successfully extracted movie genres from summaries.\n",
    "✔ Topics align with common movie genres (Action, Horror, Romance, etc.).\n",
    "✔ This approach works without labeled data and provides meaningful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Task 3 option 2: Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def lda_pipeline(documents, num_topics=5):\n",
    "    # Preprocess the documents\n",
    "    preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "    \n",
    "    # Vectorization (using TF-IDF)\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "    \n",
    "    # LDA Modeling\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, max_iter=10, learning_method='online', random_state=42)\n",
    "    lda_model.fit(tfidf_matrix)\n",
    "    \n",
    "    # Extract and print topics\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_feature_indexes = topic.argsort()[:-10 - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_feature_indexes]\n",
    "        topics.append(top_features)\n",
    "        print(f\"Topic {topic_idx+1}: {', '.join(top_features)}\")\n",
    "\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: computer, shark, accused, honor, dracula, hacker, jigsaw, altered, horde, diabolical\n",
      "Topic 2: sport, obsession, rose, sound, annie, industry, fianc, attractive, funny, challenged\n",
      "Topic 3: life, young, new, family, world, friend, man, woman, year, love\n",
      "Topic 4: unsuspecting, pacific, surfer, abusive, stalk, giovanni, bed, asterix, patriarch, hearing\n",
      "Topic 5: vega, heiress, dinosaur, ex, walter, bobby, dangerously, imprisoned, wendy, millionaire\n"
     ]
    }
   ],
   "source": [
    "# Run LDA pipeline\n",
    "topics = lda_pipeline(processed_summaries, num_topics=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
